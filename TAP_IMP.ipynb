{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0798a35f",
   "metadata": {},
   "source": [
    "DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d3863ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DAVIS Dataset Preparation\n",
      "============================================================\n",
      "\n",
      "STEP 1: Checking DAVIS dataset\n",
      "üì• Downloading DAVIS dataset (2.1GB)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 915M/915M [01:29<00:00, 10.7MiB/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Download completed!\n",
      "\n",
      "STEP 2: Extracting dataset\n",
      "üì¶ Extracting dataset...\n",
      "Extracting ./davis_data\\davis.zip to ./davis_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12607/12607 [00:05<00:00, 2482.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extraction completed!\n",
      "üßπ Cleaned up zip file\n",
      "\n",
      "STEP 3: Preparing dataset splits\n",
      "üìπ Found 90 videos in DAVIS dataset\n",
      "\n",
      "üìä Analyzing video statistics...\n",
      "   Frames per video: 50 - 90\n",
      "   Average frames: 76.8\n",
      "   Sample videos: ['bear', 'bike-packing', 'blackswan', 'bmx-bumps', 'bmx-trees', 'boat', 'boxing-fisheye', 'breakdance']\n",
      "üîÑ Processing ALL 90 videos with MAX utilization...\n",
      "\n",
      "‚öñÔ∏è Creating balanced dataset splits...\n",
      "\n",
      "üîÑ Categorizing videos by motion type...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing motion: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:00<00:00, 6923.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fast motion: 40 videos\n",
      "   Slow motion: 1 videos\n",
      "   Camera motion: 0 videos\n",
      "   Mixed motion: 49 videos\n",
      "   Target splits: Train=67, Val=15, Test=8\n",
      "   Final splits: Train=67, Val=14, Test=8\n",
      "üìÅ Created train directory: ./selected_videos\\train\n",
      "üìÅ Created val directory: ./selected_videos\\val\n",
      "üìÅ Created test directory: ./selected_videos\\test\n",
      "\n",
      "üîÑ Processing train split (67 videos, 70 frames/video)...\n",
      "   Processing color-run (1/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\color-run.npy\n",
      "   Processing scooter-black (2/67)\n",
      "     ‚úÖ Saved 43 frames to ./selected_videos\\train\\scooter-black.npy\n",
      "   Processing scooter-gray (3/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\scooter-gray.npy\n",
      "   Processing rallye (4/67)\n",
      "     ‚úÖ Saved 50 frames to ./selected_videos\\train\\rallye.npy\n",
      "   Processing car-shadow (5/67)\n",
      "     ‚úÖ Saved 40 frames to ./selected_videos\\train\\car-shadow.npy\n",
      "   Processing drone (6/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\drone.npy\n",
      "   Processing horsejump-high (7/67)\n",
      "     ‚úÖ Saved 50 frames to ./selected_videos\\train\\horsejump-high.npy\n",
      "   Processing cows (8/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\cows.npy\n",
      "   Processing varanus-cage (9/67)\n",
      "     ‚úÖ Saved 67 frames to ./selected_videos\\train\\varanus-cage.npy\n",
      "   Processing paragliding (10/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\paragliding.npy\n",
      "   Processing breakdance-flare (11/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\breakdance-flare.npy\n",
      "   Processing loading (12/67)\n",
      "     ‚úÖ Saved 50 frames to ./selected_videos\\train\\loading.npy\n",
      "   Processing dogs-jump (13/67)\n",
      "     ‚úÖ Saved 66 frames to ./selected_videos\\train\\dogs-jump.npy\n",
      "   Processing tuk-tuk (14/67)\n",
      "     ‚úÖ Saved 59 frames to ./selected_videos\\train\\tuk-tuk.npy\n",
      "   Processing car-turn (15/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\car-turn.npy\n",
      "   Processing tennis (16/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\tennis.npy\n",
      "   Processing flamingo (17/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\flamingo.npy\n",
      "   Processing dog-gooses (18/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\dog-gooses.npy\n",
      "   Processing boat (19/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\boat.npy\n",
      "   Processing motocross-bumps (20/67)\n",
      "     ‚úÖ Saved 60 frames to ./selected_videos\\train\\motocross-bumps.npy\n",
      "   Processing drift-chicane (21/67)\n",
      "     ‚úÖ Saved 52 frames to ./selected_videos\\train\\drift-chicane.npy\n",
      "   Processing longboard (22/67)\n",
      "     ‚úÖ Saved 52 frames to ./selected_videos\\train\\longboard.npy\n",
      "   Processing goat (23/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\goat.npy\n",
      "   Processing sheep (24/67)\n",
      "     ‚úÖ Saved 68 frames to ./selected_videos\\train\\sheep.npy\n",
      "   Processing mbike-trick (25/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\mbike-trick.npy\n",
      "   Processing miami-surf (26/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\miami-surf.npy\n",
      "   Processing lindy-hop (27/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\lindy-hop.npy\n",
      "   Processing swing (28/67)\n",
      "     ‚úÖ Saved 60 frames to ./selected_videos\\train\\swing.npy\n",
      "   Processing lucia (29/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\lucia.npy\n",
      "   Processing classic-car (30/67)\n",
      "     ‚úÖ Saved 63 frames to ./selected_videos\\train\\classic-car.npy\n",
      "   Processing lady-running (31/67)\n",
      "     ‚úÖ Saved 65 frames to ./selected_videos\\train\\lady-running.npy\n",
      "   Processing motocross-jump (32/67)\n",
      "     ‚úÖ Saved 40 frames to ./selected_videos\\train\\motocross-jump.npy\n",
      "   Processing scooter-board (33/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\scooter-board.npy\n",
      "   Processing dance-twirl (34/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\dance-twirl.npy\n",
      "   Processing camel (35/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\camel.npy\n",
      "   Processing drift-turn (36/67)\n",
      "     ‚úÖ Saved 64 frames to ./selected_videos\\train\\drift-turn.npy\n",
      "   Processing rollerblade (37/67)\n",
      "     ‚úÖ Saved 35 frames to ./selected_videos\\train\\rollerblade.npy\n",
      "   Processing gold-fish (38/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\gold-fish.npy\n",
      "   Processing disc-jockey (39/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\disc-jockey.npy\n",
      "   Processing snowboard (40/67)\n",
      "     ‚úÖ Saved 66 frames to ./selected_videos\\train\\snowboard.npy\n",
      "   Processing pigs (41/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\pigs.npy\n",
      "   Processing mallard-water (42/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\mallard-water.npy\n",
      "   Processing bmx-bumps (43/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\bmx-bumps.npy\n",
      "   Processing soapbox (44/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\soapbox.npy\n",
      "   Processing soccerball (45/67)\n",
      "     ‚úÖ Saved 48 frames to ./selected_videos\\train\\soccerball.npy\n",
      "   Processing crossing (46/67)\n",
      "     ‚úÖ Saved 52 frames to ./selected_videos\\train\\crossing.npy\n",
      "   Processing surf (47/67)\n",
      "     ‚úÖ Saved 55 frames to ./selected_videos\\train\\surf.npy\n",
      "   Processing shooting (48/67)\n",
      "     ‚úÖ Saved 40 frames to ./selected_videos\\train\\shooting.npy\n",
      "   Processing boxing-fisheye (49/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\boxing-fisheye.npy\n",
      "   Processing cat-girl (50/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\cat-girl.npy\n",
      "   Processing rhino (51/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\rhino.npy\n",
      "   Processing bear (52/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\bear.npy\n",
      "   Processing koala (53/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\koala.npy\n",
      "   Processing skate-park (54/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\skate-park.npy\n",
      "   Processing libby (55/67)\n",
      "     ‚úÖ Saved 49 frames to ./selected_videos\\train\\libby.npy\n",
      "   Processing car-roundabout (56/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\car-roundabout.npy\n",
      "   Processing tractor-sand (57/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\tractor-sand.npy\n",
      "   Processing train (58/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\train.npy\n",
      "   Processing stroller (59/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\stroller.npy\n",
      "   Processing lab-coat (60/67)\n",
      "     ‚úÖ Saved 47 frames to ./selected_videos\\train\\lab-coat.npy\n",
      "   Processing stunt (61/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\stunt.npy\n",
      "   Processing night-race (62/67)\n",
      "     ‚úÖ Saved 46 frames to ./selected_videos\\train\\night-race.npy\n",
      "   Processing kid-football (63/67)\n",
      "     ‚úÖ Saved 68 frames to ./selected_videos\\train\\kid-football.npy\n",
      "   Processing kite-walk (64/67)\n",
      "     ‚úÖ Saved 70 frames to ./selected_videos\\train\\kite-walk.npy\n",
      "   Processing motorbike (65/67)\n",
      "     ‚úÖ Saved 43 frames to ./selected_videos\\train\\motorbike.npy\n",
      "   Processing bike-packing (66/67)\n",
      "     ‚úÖ Saved 69 frames to ./selected_videos\\train\\bike-packing.npy\n",
      "   Processing drift-straight (67/67)\n",
      "     ‚úÖ Saved 50 frames to ./selected_videos\\train\\drift-straight.npy\n",
      "\n",
      "üîÑ Processing val split (14 videos, 40 frames/video)...\n",
      "   Processing paragliding-launch (1/14)\n",
      "     ‚úÖ Saved 40 frames to ./selected_videos\\val\\paragliding-launch.npy\n",
      "   Processing dancing (2/14)\n",
      "     ‚úÖ Saved 40 frames to ./selected_videos\\val\\dancing.npy\n",
      "   Processing upside-down (3/14)\n",
      "     ‚úÖ Saved 40 frames to ./selected_videos\\val\\upside-down.npy\n",
      "   Processing elephant (4/14)\n",
      "     ‚úÖ Saved 40 frames to ./selected_videos\\val\\elephant.npy\n",
      "   Processing breakdance (5/14)\n",
      "     ‚úÖ Saved 40 frames to ./selected_videos\\val\\breakdance.npy\n",
      "   Processing horsejump-low (6/14)\n",
      "     ‚úÖ Saved 40 frames to ./selected_videos\\val\\horsejump-low.npy\n",
      "   Processing hockey (7/14)\n",
      "     ‚úÖ Saved 40 frames to ./selected_videos\\val\\hockey.npy\n",
      "   Processing planes-water (8/14)\n",
      "     ‚úÖ Saved 38 frames to ./selected_videos\\val\\planes-water.npy\n",
      "   Processing india (9/14)\n",
      "     ‚úÖ Saved 40 frames to ./selected_videos\\val\\india.npy\n",
      "   Processing dance-jump (10/14)\n",
      "     ‚úÖ Saved 40 frames to ./selected_videos\\val\\dance-jump.npy\n",
      "   Processing parkour (11/14)\n",
      "     ‚úÖ Saved 40 frames to ./selected_videos\\val\\parkour.npy\n",
      "   Processing dog-agility (12/14)\n",
      "     ‚úÖ Saved 25 frames to ./selected_videos\\val\\dog-agility.npy\n",
      "   Processing mallard-fly (13/14)\n",
      "     ‚úÖ Saved 40 frames to ./selected_videos\\val\\mallard-fly.npy\n",
      "   Processing bus (14/14)\n",
      "     ‚úÖ Saved 40 frames to ./selected_videos\\val\\bus.npy\n",
      "\n",
      "üîÑ Processing test split (8 videos, 20 frames/video)...\n",
      "   Processing dogs-scale (1/8)\n",
      "     ‚úÖ Saved 20 frames to ./selected_videos\\test\\dogs-scale.npy\n",
      "   Processing kite-surf (2/8)\n",
      "     ‚úÖ Saved 20 frames to ./selected_videos\\test\\kite-surf.npy\n",
      "   Processing schoolgirls (3/8)\n",
      "     ‚úÖ Saved 20 frames to ./selected_videos\\test\\schoolgirls.npy\n",
      "   Processing walking (4/8)\n",
      "     ‚úÖ Saved 20 frames to ./selected_videos\\test\\walking.npy\n",
      "   Processing dog (5/8)\n",
      "     ‚úÖ Saved 20 frames to ./selected_videos\\test\\dog.npy\n",
      "   Processing hike (6/8)\n",
      "     ‚úÖ Saved 20 frames to ./selected_videos\\test\\hike.npy\n",
      "   Processing bmx-trees (7/8)\n",
      "     ‚úÖ Saved 20 frames to ./selected_videos\\test\\bmx-trees.npy\n",
      "   Processing judo (8/8)\n",
      "     ‚úÖ Saved 20 frames to ./selected_videos\\test\\judo.npy\n",
      "\n",
      "‚úÖ Dataset preparation completed!\n",
      "üìÅ Total videos saved: 89\n",
      "üéûÔ∏è Total frames saved: 4910\n",
      "üìÇ Output directory: ./selected_videos\n",
      "\n",
      "============================================================\n",
      " DATASET SUMMARY\n",
      "============================================================\n",
      "TRAIN : 67 videos, 4207 frames,  788.8 MB, 70 frames/video\n",
      "VAL   : 14 videos,  543 frames,  101.8 MB, 40 frames/video\n",
      "TEST  :  8 videos,  160 frames,   30.0 MB, 20 frames/video\n",
      "============================================================\n",
      "TOTAL : 4910 frames, 920.6 MB\n",
      "============================================================\n",
      "\n",
      "üßÆ TAP Training Sample Calculation:\n",
      "----------------------------------------\n",
      "TRAIN : 67 videos √ó 66 samples = 4422 samples\n",
      "VAL   : 14 videos √ó 36 samples = 504 samples\n",
      "TEST  : 8 videos √ó 16 samples = 128 samples\n",
      "----------------------------------------\n",
      "üéØ Total train samples for TAP: 4422\n",
      "üí° With batch_size=4: ~1105 iterations/epoch\n",
      "\n",
      "STEP 4: Verifying dataset\n",
      "\n",
      "üîç Verifying dataset...\n",
      "\n",
      "TRAIN split: 67 files\n",
      "  ‚úÖ bear.npy: (70, 256, 256, 3) (T,H,W,C)\n",
      "       Range: 0 - 254\n",
      "  ‚úÖ bike-packing.npy: (69, 256, 256, 3) (T,H,W,C)\n",
      "       Range: 3 - 255\n",
      "  ‚úÖ bmx-bumps.npy: (70, 256, 256, 3) (T,H,W,C)\n",
      "       Range: 0 - 255\n",
      "\n",
      "VAL split: 14 files\n",
      "  ‚úÖ breakdance.npy: (40, 256, 256, 3) (T,H,W,C)\n",
      "       Range: 0 - 255\n",
      "  ‚úÖ bus.npy: (40, 256, 256, 3) (T,H,W,C)\n",
      "       Range: 0 - 255\n",
      "  ‚úÖ dance-jump.npy: (40, 256, 256, 3) (T,H,W,C)\n",
      "       Range: 0 - 254\n",
      "\n",
      "TEST split: 8 files\n",
      "  ‚úÖ bmx-trees.npy: (20, 256, 256, 3) (T,H,W,C)\n",
      "       Range: 0 - 255\n",
      "  ‚úÖ dog.npy: (20, 256, 256, 3) (T,H,W,C)\n",
      "       Range: 5 - 247\n",
      "  ‚úÖ dogs-scale.npy: (20, 256, 256, 3) (T,H,W,C)\n",
      "       Range: 0 - 255\n",
      "\n",
      "üéâ All checks passed! Dataset is ready for training.\n",
      "\n",
      "‚úÖ DAVIS dataset preparation completed successfully!\n",
      "üéØ You now have the maximum possible data for TAP training.\n",
      "üìÇ Dataset location: ./selected_videos\n",
      "\n",
      "üìä FINAL STATISTICS:\n",
      "   ‚Ä¢ Using ALL available DAVIS videos\n",
      "   ‚Ä¢ Balanced splits by motion type\n",
      "   ‚Ä¢ Maximum frames per video for training\n",
      "   ‚Ä¢ Optimized for temporal denoising tasks\n",
      "   ‚Ä¢ Ready for large-scale TAP training\n"
     ]
    }
   ],
   "source": [
    "# DAVIS Dataset Preparation - MAX UTILIZATION\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "class DavisDatasetPreparer:\n",
    "    def __init__(self, download_dir=\"./davis_data\", output_dir=\"./selected_videos\"):\n",
    "        self.download_dir = download_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.davis_url = \"https://data.vision.ee.ethz.ch/csergi/share/davis/DAVIS-2017-Unsupervised-trainval-480p.zip\"\n",
    "        self.zip_path = os.path.join(download_dir, \"davis.zip\")\n",
    "        \n",
    "        # Create directories if they don't exist\n",
    "        os.makedirs(self.download_dir, exist_ok=True)\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def download_davis(self):\n",
    "        \"\"\"Download DAVIS dataset with progress bar\"\"\"\n",
    "        print(\"üì• Downloading DAVIS dataset (2.1GB)...\")\n",
    "        \n",
    "        # Check if already exists\n",
    "        davis_main_dir = os.path.join(self.download_dir, \"DAVIS\")\n",
    "        if os.path.exists(davis_main_dir):\n",
    "            print(\"‚úÖ DAVIS dataset already exists!\")\n",
    "            return True\n",
    "\n",
    "        try:\n",
    "            response = requests.get(self.davis_url, stream=True)\n",
    "            response.raise_for_status()\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "\n",
    "            with open(self.zip_path, 'wb') as file, tqdm(\n",
    "                desc=\"Downloading\",\n",
    "                total=total_size,\n",
    "                unit='iB',\n",
    "                unit_scale=True,\n",
    "                unit_divisor=1024,\n",
    "            ) as bar:\n",
    "                for data in response.iter_content(chunk_size=1024):\n",
    "                    size = file.write(data)\n",
    "                    bar.update(size)\n",
    "\n",
    "            print(\"‚úÖ Download completed!\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Download failed: {e}\")\n",
    "            print(\"üí° Please check your internet connection or download manually from:\")\n",
    "            print(\"https://davischallenge.org/davis2017/code.html\")\n",
    "            return False\n",
    "\n",
    "    def extract_dataset(self):\n",
    "        \"\"\"Extract the downloaded zip file\"\"\"\n",
    "        print(\"üì¶ Extracting dataset...\")\n",
    "\n",
    "        # Check if zip file exists before trying to extract\n",
    "        if not os.path.exists(self.zip_path):\n",
    "            print(\"‚ùå Zip file not found. Please download first.\")\n",
    "            return False\n",
    "\n",
    "        # Check if already extracted\n",
    "        davis_main_dir = os.path.join(self.download_dir, \"DAVIS\")\n",
    "        if os.path.exists(davis_main_dir):\n",
    "            print(\"‚úÖ Dataset already extracted - skipping extraction\")\n",
    "            return True\n",
    "\n",
    "        try:\n",
    "            print(f\"Extracting {self.zip_path} to {self.download_dir}...\")\n",
    "            with zipfile.ZipFile(self.zip_path, 'r') as zip_ref:\n",
    "                # Show progress for extraction\n",
    "                members = zip_ref.namelist()\n",
    "                for member in tqdm(members, desc=\"Extracting\"):\n",
    "                    zip_ref.extract(member, self.download_dir)\n",
    "            \n",
    "            print(\"‚úÖ Extraction completed!\")\n",
    "\n",
    "            # Clean up zip file to save space\n",
    "            try:\n",
    "                os.remove(self.zip_path)\n",
    "                print(\"üßπ Cleaned up zip file\")\n",
    "            except:\n",
    "                print(\"‚ö†Ô∏è Could not remove zip file\")\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Extraction failed: {e}\")\n",
    "            return False\n",
    "\n",
    "    def get_all_videos(self):\n",
    "        \"\"\"Get ALL available DAVIS videos\"\"\"\n",
    "        jpeg_dir = os.path.join(self.download_dir, \"DAVIS\", \"JPEGImages\", \"480p\")\n",
    "        if not os.path.exists(jpeg_dir):\n",
    "            raise FileNotFoundError(f\"‚ùå DAVIS JPEGImages not found at {jpeg_dir}\")\n",
    "\n",
    "        videos = sorted([d for d in os.listdir(jpeg_dir) \n",
    "                        if os.path.isdir(os.path.join(jpeg_dir, d))])\n",
    "        #verification\n",
    "        total_frames_all_videos=0\n",
    "        for video in videos:\n",
    "            video_path=os.path.join(jpeg_dir, video)\n",
    "            frame_files=sorted(glob.glob(os.path.join(video_path, \"*.jpg\")))\n",
    "            total_frames_all_videos+=len(frame_files)\n",
    "\n",
    "\n",
    "        print(f\"üìπ Found {len(videos)} videos in DAVIS dataset\")\n",
    "\n",
    "\n",
    "        # Show video statistics\n",
    "        self.analyze_video_statistics(videos, jpeg_dir)\n",
    "\n",
    "        return videos\n",
    "\n",
    "    def analyze_video_statistics(self, videos, jpeg_dir):\n",
    "        \"\"\"Analyze video statistics for better splitting\"\"\"\n",
    "        print(\"\\nüìä Analyzing video statistics...\")\n",
    "\n",
    "        video_stats = []\n",
    "        for video in videos[:min(10, len(videos))]:  # Sample first 10 for analysis\n",
    "            video_path = os.path.join(jpeg_dir, video)\n",
    "            frame_files = sorted(glob.glob(os.path.join(video_path, \"*.jpg\")))\n",
    "            if frame_files:\n",
    "                video_stats.append(len(frame_files))\n",
    "\n",
    "        if video_stats:\n",
    "            print(f\"   Frames per video: {min(video_stats)} - {max(video_stats)}\")\n",
    "            print(f\"   Average frames: {np.mean(video_stats):.1f}\")\n",
    "        else:\n",
    "            print(\"   No frames found in sampled videos\")\n",
    "\n",
    "        # Show diverse video examples\n",
    "        print(\"   Sample videos:\", videos[:min(8, len(videos))])\n",
    "\n",
    "    def categorize_videos_by_motion(self, videos, jpeg_dir):\n",
    "        \"\"\"Categorize videos by motion characteristics for balanced splits\"\"\"\n",
    "        print(\"\\nüîÑ Categorizing videos by motion type...\")\n",
    "\n",
    "        # Simple categorization based on video properties\n",
    "        fast_motion = []\n",
    "        slow_motion = []\n",
    "        camera_motion = []\n",
    "        mixed_motion = []\n",
    "\n",
    "        # Sample a few frames from each video to categorize\n",
    "        for video in tqdm(videos, desc=\"Analyzing motion\"):\n",
    "            video_path = os.path.join(jpeg_dir, video)\n",
    "            frame_files = sorted(glob.glob(os.path.join(video_path, \"*.jpg\")))\n",
    "\n",
    "            if len(frame_files) < 5:\n",
    "                mixed_motion.append(video)\n",
    "                continue\n",
    "\n",
    "            # Simple heuristic categorization\n",
    "            frame_count = len(frame_files)\n",
    "            if frame_count < 70:\n",
    "                fast_motion.append(video)\n",
    "            elif frame_count > 100:\n",
    "                slow_motion.append(video)\n",
    "            elif 'camera' in video.lower() or 'move' in video.lower():\n",
    "                camera_motion.append(video)\n",
    "            else:\n",
    "                mixed_motion.append(video)\n",
    "\n",
    "        print(f\"   Fast motion: {len(fast_motion)} videos\")\n",
    "        print(f\"   Slow motion: {len(slow_motion)} videos\")\n",
    "        print(f\"   Camera motion: {len(camera_motion)} videos\")\n",
    "        print(f\"   Mixed motion: {len(mixed_motion)} videos\")\n",
    "\n",
    "        return {\n",
    "            'fast': fast_motion,\n",
    "            'slow': slow_motion,\n",
    "            'camera': camera_motion,\n",
    "            'mixed': mixed_motion\n",
    "        }\n",
    "\n",
    "    def create_balanced_splits(self, videos, jpeg_dir, train_ratio=0.75, val_ratio=0.17, test_ratio=0.08):\n",
    "        \"\"\"Create balanced splits considering motion diversity\"\"\"\n",
    "        print(\"\\n‚öñÔ∏è Creating balanced dataset splits...\")\n",
    "\n",
    "        # Categorize videos\n",
    "        categories = self.categorize_videos_by_motion(videos, jpeg_dir)\n",
    "\n",
    "        # Calculate split sizes\n",
    "        total_videos = len(videos)\n",
    "        n_train = int(total_videos * train_ratio)\n",
    "        n_val = int(total_videos * val_ratio)\n",
    "        n_test = total_videos - n_train - n_val\n",
    "\n",
    "        print(f\"   Target splits: Train={n_train}, Val={n_val}, Test={n_test}\")\n",
    "\n",
    "        # Distribute videos from each category proportionally\n",
    "        train_videos = []\n",
    "        val_videos = []\n",
    "        test_videos = []\n",
    "\n",
    "        for category, cat_videos in categories.items():\n",
    "            cat_total = len(cat_videos)\n",
    "            if cat_total == 0:\n",
    "                continue\n",
    "                \n",
    "            cat_train = max(1, int(cat_total * train_ratio))\n",
    "            cat_val = max(1, int(cat_total * val_ratio))\n",
    "            cat_test = max(1, cat_total - cat_train - cat_val)\n",
    "\n",
    "            # Shuffle category videos\n",
    "            random.shuffle(cat_videos)\n",
    "\n",
    "            train_videos.extend(cat_videos[:cat_train])\n",
    "            val_videos.extend(cat_videos[cat_train:cat_train+cat_val])\n",
    "            test_videos.extend(cat_videos[cat_train+cat_val:cat_train+cat_val+cat_test])\n",
    "\n",
    "        # Final shuffle and trim to exact sizes\n",
    "        random.shuffle(train_videos)\n",
    "        random.shuffle(val_videos)\n",
    "        random.shuffle(test_videos)\n",
    "\n",
    "        train_videos = train_videos[:n_train]\n",
    "        val_videos = val_videos[:n_val]\n",
    "        test_videos = test_videos[:n_test]\n",
    "\n",
    "        print(f\"   Final splits: Train={len(train_videos)}, Val={len(val_videos)}, Test={len(test_videos)}\")\n",
    "\n",
    "        return train_videos, val_videos, test_videos\n",
    "\n",
    "    def load_video_frames(self, video_name, max_frames=70, resize=(256, 256)):\n",
    "        \"\"\"Load frames from a single DAVIS video - MAX frames utilization\"\"\"\n",
    "        video_path = os.path.join(self.download_dir, \"DAVIS\", \"JPEGImages\", \"480p\", video_name)\n",
    "        frame_files = sorted(glob.glob(os.path.join(video_path, \"*.jpg\")))\n",
    "\n",
    "        if len(frame_files) == 0:\n",
    "            print(f\"‚ö†Ô∏è No frames found in {video_name}\")\n",
    "            return None\n",
    "\n",
    "        # Use up to max_frames, but skip frames if video is too long to maintain temporal consistency\n",
    "        if len(frame_files) > max_frames:\n",
    "            # Sample frames evenly across the video\n",
    "            indices = np.linspace(0, len(frame_files)-1, max_frames, dtype=int)\n",
    "            frame_files = [frame_files[i] for i in indices]\n",
    "        else:\n",
    "            frame_files = frame_files[:max_frames]\n",
    "\n",
    "        frames = []\n",
    "        for frame_file in frame_files:\n",
    "            try:\n",
    "                img = Image.open(frame_file).convert('RGB')\n",
    "                if resize:\n",
    "                    img = img.resize(resize, Image.BILINEAR)\n",
    "                frames.append(np.array(img))\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error loading {frame_file}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if len(frames) == 0:\n",
    "            return None\n",
    "\n",
    "        return np.stack(frames)  # [T, H, W, 3]\n",
    "\n",
    "    def prepare_dataset_splits_max_utilization(self, frames_per_video_train=70, frames_per_video_val=40, frames_per_video_test=20):\n",
    "        \"\"\"Prepare train/val/test splits with MAXIMUM utilization of DAVIS dataset\"\"\"\n",
    "\n",
    "        # Get ALL available videos\n",
    "        try:\n",
    "            videos = self.get_all_videos()\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"‚ùå {e}\")\n",
    "            return None\n",
    "            \n",
    "        if not videos:\n",
    "            print(\"‚ùå No videos found!\")\n",
    "            return None\n",
    "\n",
    "        jpeg_dir = os.path.join(self.download_dir, \"DAVIS\", \"JPEGImages\", \"480p\")\n",
    "\n",
    "        print(f\"üîÑ Processing ALL {len(videos)} videos with MAX utilization...\")\n",
    "\n",
    "        # Create balanced splits\n",
    "        train_videos, val_videos, test_videos = self.create_balanced_splits(videos, jpeg_dir)\n",
    "\n",
    "        # Create split directories\n",
    "        split_dirs = {}\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            split_dir = os.path.join(self.output_dir, split)\n",
    "            os.makedirs(split_dir, exist_ok=True)\n",
    "            split_dirs[split] = split_dir\n",
    "            print(f\"üìÅ Created {split} directory: {split_dir}\")\n",
    "\n",
    "        # Process and save each split with optimized frame counts\n",
    "        splits = {\n",
    "            'train': (train_videos, frames_per_video_train),\n",
    "            'val': (val_videos, frames_per_video_val),\n",
    "            'test': (test_videos, frames_per_video_test)\n",
    "        }\n",
    "\n",
    "        total_saved = 0\n",
    "        total_frames = 0\n",
    "\n",
    "        for split_name, (split_videos, frames_count) in splits.items():\n",
    "            print(f\"\\nüîÑ Processing {split_name} split ({len(split_videos)} videos, {frames_count} frames/video)...\")\n",
    "\n",
    "            for i, video_name in enumerate(split_videos):\n",
    "                print(f\"   Processing {video_name} ({i+1}/{len(split_videos)})\")\n",
    "\n",
    "                try:\n",
    "                    frames = self.load_video_frames(video_name, max_frames=frames_count)\n",
    "                    if frames is not None:\n",
    "                        output_path = os.path.join(split_dirs[split_name], f\"{video_name}.npy\")\n",
    "                        np.save(output_path, frames.astype(np.uint8))\n",
    "                        total_saved += 1\n",
    "                        total_frames += frames.shape[0]\n",
    "                        print(f\"     ‚úÖ Saved {frames.shape[0]} frames to {output_path}\")\n",
    "                    else:\n",
    "                        print(f\"     ‚ùå Failed to load {video_name}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"     ‚ùå Error processing {video_name}: {e}\")\n",
    "\n",
    "        print(f\"\\n‚úÖ Dataset preparation completed!\")\n",
    "        print(f\"üìÅ Total videos saved: {total_saved}\")\n",
    "        print(f\"üéûÔ∏è Total frames saved: {total_frames}\")\n",
    "        print(f\"üìÇ Output directory: {self.output_dir}\")\n",
    "\n",
    "        # Print comprehensive summary\n",
    "        self.print_max_utilization_summary(split_dirs, splits)\n",
    "\n",
    "        # Calculate and display sample counts for training\n",
    "        self.calculate_training_samples(splits)\n",
    "\n",
    "        return split_dirs\n",
    "\n",
    "    def print_max_utilization_summary(self, split_dirs, splits):\n",
    "        \"\"\"Print comprehensive summary of the MAX utilization dataset\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\" DATASET SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        total_frames = 0\n",
    "        total_size = 0\n",
    "\n",
    "        for split_name, split_dir in split_dirs.items():\n",
    "            npy_files = glob.glob(os.path.join(split_dir, \"*.npy\"))\n",
    "            split_frames = 0\n",
    "            split_size = 0\n",
    "\n",
    "            for npy_file in npy_files:\n",
    "                try:\n",
    "                    frames = np.load(npy_file)\n",
    "                    split_frames += frames.shape[0]\n",
    "                    split_size += os.path.getsize(npy_file) / (1024 * 1024)  # MB\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            total_frames += split_frames\n",
    "            total_size += split_size\n",
    "\n",
    "            frames_per_video = splits[split_name][1]\n",
    "            print(f\"{split_name.upper():<6}: {len(npy_files):>2} videos, {split_frames:>4} frames, {split_size:>6.1f} MB, {frames_per_video} frames/video\")\n",
    "\n",
    "        print(\"=\"*60)\n",
    "        print(f\"TOTAL : {total_frames} frames, {total_size:.1f} MB\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "    def calculate_training_samples(self, splits):\n",
    "        \"\"\"Calculate training samples for TAP model\"\"\"\n",
    "        print(\"\\nüßÆ TAP Training Sample Calculation:\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # For TAP with 5-frame input, each sequence gives multiple training samples\n",
    "        tap_frame_window = 5\n",
    "\n",
    "        total_train_samples = 0\n",
    "        for split_name, (split_videos, frames_per_video) in splits.items():\n",
    "            videos_count = len(split_videos)\n",
    "            samples_per_video = max(1, frames_per_video - tap_frame_window + 1)\n",
    "            total_samples = videos_count * samples_per_video\n",
    "\n",
    "            if split_name == 'train':\n",
    "                total_train_samples = total_samples\n",
    "\n",
    "            print(f\"{split_name.upper():<6}: {videos_count} videos √ó {samples_per_video} samples = {total_samples} samples\")\n",
    "\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"üéØ Total train samples for TAP: {total_train_samples}\")\n",
    "        print(\"üí° With batch_size=4: ~{} iterations/epoch\".format(total_train_samples // 4))\n",
    "\n",
    "    def verify_dataset(self):\n",
    "        \"\"\"Verify the prepared dataset\"\"\"\n",
    "        print(\"\\nüîç Verifying dataset...\")\n",
    "\n",
    "        splits = ['train', 'val', 'test']\n",
    "        all_good = True\n",
    "\n",
    "        for split in splits:\n",
    "            split_dir = os.path.join(self.output_dir, split)\n",
    "            npy_files = glob.glob(os.path.join(split_dir, \"*.npy\"))\n",
    "\n",
    "            print(f\"\\n{split.upper()} split: {len(npy_files)} files\")\n",
    "\n",
    "            if len(npy_files) == 0:\n",
    "                print(f\"  ‚ö†Ô∏è No .npy files found in {split_dir}\")\n",
    "                all_good = False\n",
    "                continue\n",
    "\n",
    "            for npy_file in npy_files[:min(3, len(npy_files))]:  # Check first 3 files\n",
    "                try:\n",
    "                    frames = np.load(npy_file)\n",
    "                    print(f\"  ‚úÖ {os.path.basename(npy_file)}: {frames.shape} (T,H,W,C)\")\n",
    "\n",
    "                    # Verify frame content\n",
    "                    if frames.shape[0] > 0 and frames.shape[1] > 0 and frames.shape[2] > 0:\n",
    "                        print(f\"       Range: {frames.min()} - {frames.max()}\")\n",
    "                    else:\n",
    "                        print(f\"  ‚ùå Invalid frame dimensions\")\n",
    "                        all_good = False\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ùå {os.path.basename(npy_file)}: Error - {e}\")\n",
    "                    all_good = False\n",
    "\n",
    "        if all_good:\n",
    "            print(\"\\nüéâ All checks passed! Dataset is ready for training.\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è Some issues found. Please check the dataset.\")\n",
    "\n",
    "        return all_good\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to prepare DAVIS dataset with MAX utilization\"\"\"\n",
    "    print(\" DAVIS Dataset Preparation\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Initialize preparer with relative paths for VS Code\n",
    "    preparer = DavisDatasetPreparer(\n",
    "        download_dir=\"./davis_data\",\n",
    "        output_dir=\"./selected_videos\"\n",
    "    )\n",
    "\n",
    "    # Step 1: Download DAVIS ONLY if needed\n",
    "    print(\"\\nSTEP 1: Checking DAVIS dataset\")\n",
    "    davis_path = os.path.join(preparer.download_dir, \"DAVIS\")\n",
    "    if os.path.exists(davis_path):\n",
    "        print(\"‚úÖ DAVIS dataset already exists - skipping download\")\n",
    "    else:\n",
    "        success = preparer.download_davis()\n",
    "        if not success:\n",
    "            print(\"‚ùå Download failed. Exiting.\")\n",
    "            return\n",
    "\n",
    "    # Step 2: Extract dataset (will skip if already extracted)\n",
    "    print(\"\\nSTEP 2: Extracting dataset\")\n",
    "    extract_success = preparer.extract_dataset()\n",
    "    if not extract_success:\n",
    "        print(\"‚ùå Extraction failed. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Step 3: Prepare splits with MAX utilization\n",
    "    print(\"\\nSTEP 3: Preparing dataset splits\")\n",
    "    split_dirs = preparer.prepare_dataset_splits_max_utilization(\n",
    "        frames_per_video_train=70,    # Maximum frames for training\n",
    "        frames_per_video_val=40,      # Good amount for validation\n",
    "        frames_per_video_test=20      # Sufficient for testing\n",
    "    )\n",
    "\n",
    "    if not split_dirs:\n",
    "        print(\"‚ùå Failed to prepare dataset splits\")\n",
    "        return\n",
    "\n",
    "    # Step 4: Verify dataset\n",
    "    print(\"\\nSTEP 4: Verifying dataset\")\n",
    "    preparer.verify_dataset()\n",
    "\n",
    "    print(\"\\n‚úÖ DAVIS dataset preparation completed successfully!\")\n",
    "    print(\"üéØ You now have the maximum possible data for TAP training.\")\n",
    "    print(f\"üìÇ Dataset location: {preparer.output_dir}\")\n",
    "\n",
    "    # Final statistics\n",
    "    print(\"\\nüìä FINAL STATISTICS:\")\n",
    "    print(\"   ‚Ä¢ Using ALL available DAVIS videos\")\n",
    "    print(\"   ‚Ä¢ Balanced splits by motion type\")\n",
    "    print(\"   ‚Ä¢ Maximum frames per video for training\")\n",
    "    print(\"   ‚Ä¢ Optimized for temporal denoising tasks\")\n",
    "    print(\"   ‚Ä¢ Ready for large-scale TAP training\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81982789",
   "metadata": {},
   "source": [
    "TAP IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d94ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 1\n",
      "Current GPU: 0\n",
      "GPU name: NVIDIA GeForce RTX 4070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Method 1.1: Simple check\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Method 1.2: Check GPU count\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Method 1.3: Get current GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945524f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ GPU is available! Training will use CUDA.\n",
      "\n",
      "==================================================\n",
      "TRUE TAP Video Denoising Pipeline - VS CODE\n",
      "==================================================\n",
      "Flow: Noisy ‚Üí Pre-trained Denoiser ‚Üí TAP + Motion Attention ‚Üí Self-supervised ‚Üí Denoised\n",
      "==================================================\n",
      "            base_channels: 16\n",
      "               batch_size: 2\n",
      "               davis_root: ./davis_data/DAVIS/JPEGImages/480p\n",
      "               dec_blocks: [3, 3, 2]\n",
      "               enc_blocks: [2, 3, 3]\n",
      "                   epochs: 15\n",
      "             eval_metrics: False\n",
      "                eval_step: 5\n",
      "         frames_per_video: 70\n",
      "                       lr: 0.0001\n",
      "                     mode: finetune\n",
      "                 n_frames: 5\n",
      "          nafnet_channels: 48\n",
      "              noise_sigma: 25\n",
      "              num_workers: 0\n",
      "               patch_size: 256\n",
      "          pretrained_path: \n",
      "                 save_dir: ./output_models\n",
      "             save_visuals: False\n",
      "          self_supervised: True\n",
      "               test_ratio: 0.08\n",
      "              train_ratio: 0.75\n",
      "     use_motion_attention: True\n",
      "                val_ratio: 0.17\n",
      "            videos_to_use: 55\n",
      "==================================================\n",
      "\n",
      "üöÄ Using device: cuda\n",
      "üéØ GPU: NVIDIA GeForce RTX 4070\n",
      "üéØ GPU Memory: 12.0 GB\n",
      "üîÑ Initializing TRUE TAP model...\n",
      "ü§ñ TRUE TAP Model Analysis:\n",
      "   Total parameters: 12,569,403\n",
      "   TAP plugins (trainable): 8,821,032\n",
      "   NAFNet backbone (frozen): 3,748,371\n",
      "   Plugin ratio: 70.2%\n",
      "   Architecture: Deformable Alignment + Temporal Attention\n",
      "üß™ Testing TRUE TAP forward pass...\n",
      "‚úÖ TRUE TAP forward pass successful!\n",
      "   Input: torch.Size([2, 5, 3, 256, 256]) ‚Üí Output: torch.Size([2, 3, 256, 256])\n",
      "   GPU Memory used: 0.02 GB\n",
      "üìä Creating dataloaders...\n",
      "üìÅ Loaded 67 videos from ./selected_videos/train\n",
      "üìÅ Loaded 14 videos from ./selected_videos/val\n",
      "‚úÖ Train: 2010 samples\n",
      "‚úÖ Val: 140 samples\n",
      "üöÄ Starting TRUE TAP training for 15 epochs...\n",
      "üí° Only TAP temporal plugins are being trained\n",
      "üéØ Flow: Noisy ‚Üí Pre-trained Denoiser ‚Üí TAP ‚Üí Denoised\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1005/1005 [07:33<00:00,  2.22it/s, loss=0.0995, lr=1.00e-04]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [00:16<00:00,  4.18it/s, val_loss=0.0972]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch 1/15: Train Loss: 0.1135, Val Loss: 0.0958, Time: 469.8s\n",
      "üíæ Saved model to true_tap_best.pth\n",
      "   - TAP parameters: 80\n",
      "   - Total parameters: 273\n",
      "   - Best validation loss: 0.0958\n",
      "üèÜ New best model! Val Loss: 0.0958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1005/1005 [07:26<00:00,  2.25it/s, loss=0.0797, lr=1.00e-04]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [00:16<00:00,  4.25it/s, val_loss=0.0914]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch 2/15: Train Loss: 0.0925, Val Loss: 0.0909, Time: 462.7s\n",
      "üíæ Saved model to true_tap_best.pth\n",
      "   - TAP parameters: 80\n",
      "   - Total parameters: 273\n",
      "   - Best validation loss: 0.0909\n",
      "üèÜ New best model! Val Loss: 0.0909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1005/1005 [07:26<00:00,  2.25it/s, loss=0.0878, lr=1.00e-04]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [00:16<00:00,  4.24it/s, val_loss=0.0903]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch 3/15: Train Loss: 0.0896, Val Loss: 0.0905, Time: 463.1s\n",
      "üíæ Saved model to true_tap_best.pth\n",
      "   - TAP parameters: 80\n",
      "   - Total parameters: 273\n",
      "   - Best validation loss: 0.0905\n",
      "üèÜ New best model! Val Loss: 0.0905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1005/1005 [07:26<00:00,  2.25it/s, loss=0.0864, lr=1.00e-04]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [00:16<00:00,  4.23it/s, val_loss=0.0882]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch 4/15: Train Loss: 0.0887, Val Loss: 0.0886, Time: 462.8s\n",
      "üíæ Saved model to true_tap_best.pth\n",
      "   - TAP parameters: 80\n",
      "   - Total parameters: 273\n",
      "   - Best validation loss: 0.0886\n",
      "üèÜ New best model! Val Loss: 0.0886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1005/1005 [07:26<00:00,  2.25it/s, loss=0.0965, lr=1.00e-04]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [00:16<00:00,  4.28it/s, val_loss=0.0873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch 5/15: Train Loss: 0.0878, Val Loss: 0.0882, Time: 462.7s\n",
      "üíæ Saved model to true_tap_best.pth\n",
      "   - TAP parameters: 80\n",
      "   - Total parameters: 273\n",
      "   - Best validation loss: 0.0882\n",
      "üèÜ New best model! Val Loss: 0.0882\n",
      "üíæ Saved TAP checkpoint at epoch 5\n",
      "üìä Training history plot saved: ./output_models\\training_history.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1005/1005 [07:25<00:00,  2.25it/s, loss=0.0762, lr=1.00e-04]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [00:16<00:00,  4.32it/s, val_loss=0.0865]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch 6/15: Train Loss: 0.0876, Val Loss: 0.0871, Time: 462.1s\n",
      "üíæ Saved model to true_tap_best.pth\n",
      "   - TAP parameters: 80\n",
      "   - Total parameters: 273\n",
      "   - Best validation loss: 0.0871\n",
      "üèÜ New best model! Val Loss: 0.0871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1005/1005 [07:26<00:00,  2.25it/s, loss=0.0926, lr=1.00e-04]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [00:16<00:00,  4.23it/s, val_loss=0.0866]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch 7/15: Train Loss: 0.0866, Val Loss: 0.0870, Time: 462.7s\n",
      "üíæ Saved model to true_tap_best.pth\n",
      "   - TAP parameters: 80\n",
      "   - Total parameters: 273\n",
      "   - Best validation loss: 0.0870\n",
      "üèÜ New best model! Val Loss: 0.0870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1005/1005 [19:12<00:00,  1.15s/it, loss=0.0925, lr=1.00e-04]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [00:51<00:00,  1.37it/s, val_loss=0.0856]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch 8/15: Train Loss: 0.0863, Val Loss: 0.0865, Time: 1203.4s\n",
      "üíæ Saved model to true_tap_best.pth\n",
      "   - TAP parameters: 80\n",
      "   - Total parameters: 273\n",
      "   - Best validation loss: 0.0865\n",
      "üèÜ New best model! Val Loss: 0.0865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1005/1005 [53:53<00:00,  3.22s/it, loss=0.0845, lr=1.00e-04]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [00:50<00:00,  1.38it/s, val_loss=0.0857]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch 9/15: Train Loss: 0.0866, Val Loss: 0.0865, Time: 3284.5s\n",
      "üíæ Saved model to true_tap_best.pth\n",
      "   - TAP parameters: 80\n",
      "   - Total parameters: 273\n",
      "   - Best validation loss: 0.0865\n",
      "üèÜ New best model! Val Loss: 0.0865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1005/1005 [53:53<00:00,  3.22s/it, loss=0.0795, lr=1.00e-04]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [00:50<00:00,  1.38it/s, val_loss=0.0856]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch 10/15: Train Loss: 0.0859, Val Loss: 0.0862, Time: 3284.3s\n",
      "üíæ Saved model to true_tap_best.pth\n",
      "   - TAP parameters: 80\n",
      "   - Total parameters: 273\n",
      "   - Best validation loss: 0.0862\n",
      "üèÜ New best model! Val Loss: 0.0862\n",
      "üíæ Saved TAP checkpoint at epoch 10\n",
      "üìä Training history plot saved: ./output_models\\training_history.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1005/1005 [1:19:17<00:00,  4.73s/it, loss=0.0720, lr=1.00e-04]    \n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [00:16<00:00,  4.24it/s, val_loss=0.0852]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch 11/15: Train Loss: 0.0858, Val Loss: 0.0862, Time: 4774.0s\n",
      "üíæ Saved model to true_tap_best.pth\n",
      "   - TAP parameters: 80\n",
      "   - Total parameters: 273\n",
      "   - Best validation loss: 0.0862\n",
      "üèÜ New best model! Val Loss: 0.0862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1005/1005 [08:03<00:00,  2.08it/s, loss=0.0747, lr=1.00e-04]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [00:21<00:00,  3.22it/s, val_loss=0.0847]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch 12/15: Train Loss: 0.0860, Val Loss: 0.0856, Time: 504.9s\n",
      "üíæ Saved model to true_tap_best.pth\n",
      "   - TAP parameters: 80\n",
      "   - Total parameters: 273\n",
      "   - Best validation loss: 0.0856\n",
      "üèÜ New best model! Val Loss: 0.0856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1005/1005 [16:39<00:00,  1.01it/s, loss=0.0821, lr=1.00e-04]  \n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [00:16<00:00,  4.29it/s, val_loss=0.0848]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch 13/15: Train Loss: 0.0852, Val Loss: 0.0855, Time: 1015.6s\n",
      "üíæ Saved model to true_tap_best.pth\n",
      "   - TAP parameters: 80\n",
      "   - Total parameters: 273\n",
      "   - Best validation loss: 0.0855\n",
      "üèÜ New best model! Val Loss: 0.0855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1005/1005 [07:27<00:00,  2.24it/s, loss=0.0889, lr=1.00e-04]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [00:16<00:00,  4.26it/s, val_loss=0.0845]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch 14/15: Train Loss: 0.0853, Val Loss: 0.0853, Time: 464.2s\n",
      "üíæ Saved model to true_tap_best.pth\n",
      "   - TAP parameters: 80\n",
      "   - Total parameters: 273\n",
      "   - Best validation loss: 0.0853\n",
      "üèÜ New best model! Val Loss: 0.0853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1005/1005 [07:27<00:00,  2.25it/s, loss=0.0861, lr=1.00e-04]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [00:16<00:00,  4.24it/s, val_loss=0.0845]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Epoch 15/15: Train Loss: 0.0853, Val Loss: 0.0856, Time: 464.1s\n",
      "üíæ Saved TAP checkpoint at epoch 15\n",
      "üìä Training history plot saved: ./output_models\\training_history.png\n",
      "üìä Training history plot saved: ./output_models\\training_history.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Training data saved: ./output_models\\training_data.npy\n",
      "üíæ Saved model to true_tap_final.pth\n",
      "   - TAP parameters: 80\n",
      "   - Total parameters: 273\n",
      "üéâ TRUE TAP training completed!\n",
      "üìÅ Final TAP weights saved as: ./output_models/true_tap_final.pth\n",
      "üèÜ Best validation loss: 0.0853\n",
      "üìä Training curves saved as: ./output_models/training_history.png\n",
      "‚úÖ TRUE TAP training completed successfully!\n",
      "üéØ Correct Flow Implemented: Noisy ‚Üí Pre-trained Denoiser ‚Üí TAP ‚Üí Denoised\n",
      "\n",
      "üìä FINAL TRAINING SUMMARY:\n",
      "   Initial Train Loss: 0.1135\n",
      "   Final Train Loss: 0.0853\n",
      "   Final Val Loss: 0.0856\n",
      "   Best Val Loss: 0.0853\n",
      "   Total Improvement: 0.0282\n",
      "   Overfitting: 0.0003\n"
     ]
    }
   ],
   "source": [
    "# TRUE TAP Pipeline with CORRECT Flow and Parameters - VS CODE VERSION\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Windows multiprocessing fix\n",
    "if os.name == 'nt':\n",
    "    import multiprocessing\n",
    "    multiprocessing.freeze_support()\n",
    "\n",
    "# -------------------------\n",
    "# Configuration (UPDATED for VS Code)\n",
    "# -------------------------\n",
    "class TAPOptions:\n",
    "    def __init__(self):\n",
    "        self.mode = 'finetune'\n",
    "        self.davis_root = './davis_data/DAVIS/JPEGImages/480p'  # Updated path\n",
    "        self.save_dir = './output_models'\n",
    "        self.pretrained_path = ''\n",
    "        self.n_frames = 5\n",
    "        self.patch_size = 256\n",
    "        self.noise_sigma = 25\n",
    "        self.videos_to_use = 55\n",
    "        self.frames_per_video = 70\n",
    "        self.batch_size = 2\n",
    "        self.epochs = 15\n",
    "        self.lr = 1e-4\n",
    "        self.num_workers = 0  # Reduced for VS Code stability\n",
    "        self.base_channels = 16\n",
    "        self.enc_blocks = [2, 3, 3]\n",
    "        self.dec_blocks = [3, 3, 2]\n",
    "        self.eval_metrics = False\n",
    "        self.eval_step = 5\n",
    "        self.save_visuals = False\n",
    "        self.train_ratio = 0.75\n",
    "        self.val_ratio = 0.17\n",
    "        self.test_ratio = 0.08\n",
    "        self.nafnet_channels = 48\n",
    "        self.use_motion_attention = True\n",
    "        self.self_supervised = True\n",
    "\n",
    "    def print_options(self):\n",
    "        message = '\\n' + '='*50 + '\\n'\n",
    "        message += 'TRUE TAP Video Denoising Pipeline - VS CODE\\n'\n",
    "        message += '='*50 + '\\n'\n",
    "        message += 'Flow: Noisy ‚Üí Pre-trained Denoiser ‚Üí TAP + Motion Attention ‚Üí Self-supervised ‚Üí Denoised\\n'\n",
    "        message += '='*50 + '\\n'\n",
    "\n",
    "        for key, value in sorted(vars(self).items()):\n",
    "            message += f'{key:>25}: {value}\\n'\n",
    "\n",
    "        message += '='*50 + '\\n'\n",
    "        print(message)\n",
    "\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        with open(os.path.join(self.save_dir, 'config.txt'), 'w', encoding='utf-8') as f:\n",
    "            f.write(message)\n",
    "\n",
    "        return message\n",
    "\n",
    "# -------------------------\n",
    "# NAFNet Components\n",
    "# -------------------------\n",
    "class SimpleChannelAttention(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        bottleneck = max(channel // 8, 4)\n",
    "        self.ca = nn.Sequential(\n",
    "            nn.Conv2d(channel, bottleneck, 1, padding=0, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(bottleneck, channel, 1, padding=0, bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)\n",
    "        y = self.ca(y)\n",
    "        return x * y\n",
    "\n",
    "class SimpleNAFBlock(nn.Module):\n",
    "    def __init__(self, c, DW_Expand=2):\n",
    "        super().__init__()\n",
    "        dw_channel = c * DW_Expand\n",
    "        self.conv1 = nn.Conv2d(c, dw_channel, 1, padding=0, bias=True)\n",
    "        self.conv2 = nn.Conv2d(dw_channel, dw_channel, 3, padding=1, groups=dw_channel, bias=True)\n",
    "        self.conv3 = nn.Conv2d(dw_channel, c, 1, padding=0, bias=True)\n",
    "        self.sca = SimpleChannelAttention(dw_channel)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = self.conv1(inp)\n",
    "        x = self.act(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.sca(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv3(x)\n",
    "        return x + inp\n",
    "\n",
    "class PretrainedNAFNet(nn.Module):\n",
    "    def __init__(self, img_channel=3, width=48, enc_blks=[2, 3, 3], dec_blks=[3, 3, 2]):\n",
    "        super().__init__()\n",
    "        self.img_channel = img_channel\n",
    "        self.width = width\n",
    "\n",
    "        # Enhanced encoder with 4 levels\n",
    "        self.enc_conv0 = nn.Conv2d(img_channel, width, 3, padding=1, bias=True)\n",
    "        self.enc_conv1 = nn.Sequential(*[SimpleNAFBlock(width) for _ in range(enc_blks[0])])\n",
    "        self.down1 = nn.Conv2d(width, width*2, 2, 2)\n",
    "        self.enc_conv2 = nn.Sequential(*[SimpleNAFBlock(width*2) for _ in range(enc_blks[1])])\n",
    "        self.down2 = nn.Conv2d(width*2, width*4, 2, 2)\n",
    "        self.enc_conv3 = nn.Sequential(*[SimpleNAFBlock(width*4) for _ in range(enc_blks[2])])\n",
    "        self.down3 = nn.Conv2d(width*4, width*8, 2, 2)\n",
    "\n",
    "        # Enhanced middle\n",
    "        self.middle = nn.Sequential(*[SimpleNAFBlock(width*8) for _ in range(2)])\n",
    "\n",
    "        # Enhanced decoder with 4 levels\n",
    "        self.up3 = nn.Sequential(\n",
    "            nn.Conv2d(width*8, width*4*4, 1, bias=False),\n",
    "            nn.PixelShuffle(2)\n",
    "        )\n",
    "        self.dec_conv3 = nn.Sequential(*[SimpleNAFBlock(width*4) for _ in range(dec_blks[0])])\n",
    "\n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.Conv2d(width*4, width*2*4, 1, bias=False),\n",
    "            nn.PixelShuffle(2)\n",
    "        )\n",
    "        self.dec_conv2 = nn.Sequential(*[SimpleNAFBlock(width*2) for _ in range(dec_blks[1])])\n",
    "\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.Conv2d(width*2, width*4, 1, bias=False),\n",
    "            nn.PixelShuffle(2)\n",
    "        )\n",
    "        self.dec_conv1 = nn.Sequential(*[SimpleNAFBlock(width) for _ in range(dec_blks[2])])\n",
    "        self.output_conv = nn.Conv2d(width, img_channel, 3, padding=1, bias=True)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "        # Freeze all parameters\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Full denoising forward pass\"\"\"\n",
    "        # Encoder\n",
    "        x0 = self.enc_conv0(x)\n",
    "        x1 = self.enc_conv1(x0)\n",
    "        x1_down = self.down1(x1)\n",
    "        x2 = self.enc_conv2(x1_down)\n",
    "        x2_down = self.down2(x2)\n",
    "        x3 = self.enc_conv3(x2_down)\n",
    "        x3_down = self.down3(x3)\n",
    "\n",
    "        # Middle\n",
    "        middle_out = self.middle(x3_down)\n",
    "\n",
    "        # Decoder\n",
    "        x3_up = self.up3(middle_out)\n",
    "        x3_cat = x3 + x3_up\n",
    "        x3_out = self.dec_conv3(x3_cat)\n",
    "\n",
    "        x2_up = self.up2(x3_out)\n",
    "        x2_cat = x2 + x2_up\n",
    "        x2_out = self.dec_conv2(x2_cat)\n",
    "\n",
    "        x1_up = self.up1(x2_out)\n",
    "        x1_cat = x1 + x1_up\n",
    "        x1_out = self.dec_conv1(x1_cat)\n",
    "\n",
    "        out = self.output_conv(x1_out + x0)\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "    def get_skip_features(self, x):\n",
    "        \"\"\"Extract features for TAP fusion\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x0 = self.enc_conv0(x)\n",
    "            x1 = self.enc_conv1(x0)\n",
    "            x1_down = self.down1(x1)\n",
    "            x2 = self.enc_conv2(x1_down)\n",
    "            x2_down = self.down2(x2)\n",
    "            x3 = self.enc_conv3(x2_down)\n",
    "        return x0, x1, x2, x3\n",
    "\n",
    "# -------------------------\n",
    "# TRUE TAP TEMPORAL MODULES\n",
    "# -------------------------\n",
    "class DeformableConv2d(nn.Module):\n",
    "    \"\"\"Deformable convolution for temporal alignment\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "\n",
    "        self.offset_conv = nn.Conv2d(in_channels * 2, 2 * kernel_size * kernel_size,\n",
    "                                   kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "                            padding=padding, bias=True)\n",
    "\n",
    "        self.offset_conv.weight.data.zero_()\n",
    "        self.offset_conv.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, ref_feat):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        offset_input = torch.cat([ref_feat, x], dim=1)\n",
    "        offsets = self.offset_conv(offset_input)\n",
    "\n",
    "        aligned = self.deform_conv2d(x, offsets, self.conv)\n",
    "        return aligned\n",
    "\n",
    "    def deform_conv2d(self, x, offsets, weight_conv):\n",
    "        B, C, H, W = x.shape\n",
    "        kh, kw = self.kernel_size, self.kernel_size\n",
    "\n",
    "        y_coords, x_coords = torch.meshgrid(\n",
    "            torch.arange(H, device=x.device, dtype=torch.float32),\n",
    "            torch.arange(W, device=x.device, dtype=torch.float32),\n",
    "            indexing='ij'\n",
    "        )\n",
    "\n",
    "        y_coords = 2.0 * y_coords / (H - 1) - 1.0\n",
    "        x_coords = 2.0 * x_coords / (W - 1) - 1.0\n",
    "\n",
    "        grid = torch.stack([x_coords, y_coords], dim=-1).unsqueeze(0).repeat(B, 1, 1, 1)\n",
    "\n",
    "        offsets = offsets.permute(0, 2, 3, 1).reshape(B, H, W, kh * kw, 2)\n",
    "        offsets = offsets * 0.1\n",
    "\n",
    "        sampling_grids = []\n",
    "        for i in range(kh):\n",
    "            for j in range(kw):\n",
    "                ky = (i - kh // 2) * 2.0 / H\n",
    "                kx = (j - kw // 2) * 2.0 / W\n",
    "\n",
    "                kernel_offset = torch.tensor([kx, ky], device=x.device).view(1, 1, 1, 2)\n",
    "                idx = i * kw + j\n",
    "                total_offset = grid + kernel_offset + offsets[:, :, :, idx, :]\n",
    "                sampling_grids.append(total_offset)\n",
    "\n",
    "        sampling_grid = torch.stack(sampling_grids, dim=3)\n",
    "        sampling_grid = sampling_grid.reshape(B, H, W * kh * kw, 2)\n",
    "\n",
    "        x_sampled = F.grid_sample(\n",
    "            x,\n",
    "            sampling_grid,\n",
    "            mode='bilinear',\n",
    "            padding_mode='zeros',\n",
    "            align_corners=False\n",
    "        )\n",
    "\n",
    "        x_sampled = x_sampled.view(B, C, H, W * kh * kw)\n",
    "        x_sampled = x_sampled.view(B, C, H, W, kh * kw)\n",
    "        x_sampled = x_sampled.permute(0, 1, 4, 2, 3)\n",
    "\n",
    "        weight = weight_conv.weight\n",
    "        weight = weight.view(weight_conv.out_channels, C, kh * kw)\n",
    "\n",
    "        x_sampled = x_sampled.reshape(B, C, kh * kw, H * W)\n",
    "        output = torch.einsum('ock,bckn->bon', weight, x_sampled)\n",
    "        output = output.view(B, weight_conv.out_channels, H, W)\n",
    "\n",
    "        if weight_conv.bias is not None:\n",
    "            output += weight_conv.bias.view(1, -1, 1, 1)\n",
    "\n",
    "        return output\n",
    "\n",
    "class DeformableAlignment(nn.Module):\n",
    "    \"\"\"TAP Paper: Deformable convolution alignment\"\"\"\n",
    "    def __init__(self, channels, n_frames=5):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.n_frames = n_frames\n",
    "\n",
    "        self.deform_conv = DeformableConv2d(channels, channels)\n",
    "\n",
    "        self.align_feat_extract = nn.Sequential(\n",
    "            nn.Conv2d(channels * 2, channels, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, ref_feat, neighbor_feat):\n",
    "        aligned_feat = self.deform_conv(neighbor_feat, ref_feat)\n",
    "        aligned_feat = aligned_feat + neighbor_feat\n",
    "        return aligned_feat\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    \"\"\"TAP Paper: Motion-aware temporal attention\"\"\"\n",
    "    def __init__(self, channels, n_frames=5):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.n_frames = n_frames\n",
    "\n",
    "        self.temporal_att = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels // 4, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels // 4, channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.motion_conv = nn.Sequential(\n",
    "            nn.Conv2d(channels * 2, channels, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, aligned_feats):\n",
    "        B, T, C, H, W = aligned_feats.shape\n",
    "\n",
    "        ref_feat = aligned_feats[:, T//2]\n",
    "        motion_features = []\n",
    "\n",
    "        for t in range(T):\n",
    "            if t != T//2:\n",
    "                motion = torch.cat([ref_feat, aligned_feats[:, t]], dim=1)\n",
    "                motion_feat = self.motion_conv(motion)\n",
    "                motion_features.append(motion_feat.unsqueeze(1))\n",
    "\n",
    "        motion_features = torch.cat(motion_features, dim=1)\n",
    "\n",
    "        temporal_weights = self.temporal_att(\n",
    "            motion_features.view(B * (T-1), C, H, W)\n",
    "        ).view(B, T-1, C, 1, 1)\n",
    "\n",
    "        weighted_features = []\n",
    "        idx = 0\n",
    "        for t in range(T):\n",
    "            if t == T//2:\n",
    "                weighted_features.append(aligned_feats[:, t].unsqueeze(1))\n",
    "            else:\n",
    "                weighted_feat = aligned_feats[:, t] * temporal_weights[:, idx]\n",
    "                weighted_features.append(weighted_feat.unsqueeze(1))\n",
    "                idx += 1\n",
    "\n",
    "        return torch.cat(weighted_features, dim=1)\n",
    "\n",
    "class TAPTemporalPlugin(nn.Module):\n",
    "    \"\"\"TRUE TAP: Complete temporal plugin with deformable alignment\"\"\"\n",
    "    def __init__(self, channels, n_frames=5):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.n_frames = n_frames\n",
    "        self.half_frames = n_frames // 2\n",
    "\n",
    "        self.deform_align = DeformableAlignment(channels, n_frames)\n",
    "        self.temporal_att = TemporalAttention(channels, n_frames)\n",
    "\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(channels * n_frames, channels * 2, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels * 2, channels, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, ref_feat, neighbor_feats):\n",
    "        B, C, H, W = ref_feat.shape\n",
    "        T = len(neighbor_feats) + 1\n",
    "\n",
    "        aligned_feats = [ref_feat.unsqueeze(1)]\n",
    "\n",
    "        for neighbor_feat in neighbor_feats:\n",
    "            aligned_neighbor = self.deform_align(ref_feat, neighbor_feat)\n",
    "            aligned_feats.append(aligned_neighbor.unsqueeze(1))\n",
    "\n",
    "        aligned_feats = torch.cat(aligned_feats, dim=1)\n",
    "        attended_feats = self.temporal_att(aligned_feats)\n",
    "\n",
    "        fused_feats = attended_feats.view(B, T * C, H, W)\n",
    "        output = self.fusion(fused_feats) + ref_feat\n",
    "\n",
    "        return output\n",
    "\n",
    "class TrueTAPWithNAFNet(nn.Module):\n",
    "    \"\"\"TRUE TAP with CORRECT flow: Noisy ‚Üí Pre-trained Denoiser ‚Üí TAP ‚Üí Denoised\"\"\"\n",
    "    def __init__(self, n_frames=5, img_channel=3, width=48, enc_blks=[2, 3, 3], dec_blks=[3, 3, 2]):\n",
    "        super().__init__()\n",
    "        self.n_frames = n_frames\n",
    "        self.half_frames = n_frames // 2\n",
    "\n",
    "        # Pre-trained NAFNet as IMAGE DENOISER\n",
    "        self.nafnet = PretrainedNAFNet(img_channel, width, enc_blks, dec_blks)\n",
    "\n",
    "        # TRUE TAP temporal plugins\n",
    "        self.tap_plugin0 = TAPTemporalPlugin(width, n_frames)\n",
    "        self.tap_plugin1 = TAPTemporalPlugin(width, n_frames)\n",
    "        self.tap_plugin2 = TAPTemporalPlugin(width*2, n_frames)\n",
    "        self.tap_plugin3 = TAPTemporalPlugin(width*4, n_frames)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        \"\"\"\n",
    "        CORRECT FLOW:\n",
    "        1. Noisy video frames input\n",
    "        2. Apply pre-trained denoiser to each frame\n",
    "        3. Extract features from denoised frames\n",
    "        4. Apply TAP temporal fusion\n",
    "        5. Output denoised video frame\n",
    "        \"\"\"\n",
    "        B, T, C, H, W = inp.shape\n",
    "\n",
    "        # Step 1: Apply pre-trained denoiser to each frame\n",
    "        denoised_frames = []\n",
    "        for t in range(T):\n",
    "            frame_denoised = self.nafnet(inp[:, t])\n",
    "            denoised_frames.append(frame_denoised)\n",
    "\n",
    "        # Stack denoised frames\n",
    "        denoised_sequence = torch.stack(denoised_frames, dim=1)\n",
    "\n",
    "        # Step 2: Extract reference frame and neighbors from DENOISED sequence\n",
    "        ref_frame = denoised_sequence[:, T//2]\n",
    "        neighbor_frames = []\n",
    "        for i in range(T):\n",
    "            if i != T//2:\n",
    "                neighbor_frames.append(denoised_sequence[:, i])\n",
    "\n",
    "        # Step 3: Extract skip features from DENOISED frames\n",
    "        ref_x0, ref_x1, ref_x2, ref_x3 = self.nafnet.get_skip_features(ref_frame)\n",
    "\n",
    "        neighbor_x0s, neighbor_x1s, neighbor_x2s, neighbor_x3s = [], [], [], []\n",
    "        for frame in neighbor_frames:\n",
    "            n_x0, n_x1, n_x2, n_x3 = self.nafnet.get_skip_features(frame)\n",
    "            neighbor_x0s.append(n_x0)\n",
    "            neighbor_x1s.append(n_x1)\n",
    "            neighbor_x2s.append(n_x2)\n",
    "            neighbor_x3s.append(n_x3)\n",
    "\n",
    "        # Step 4: Apply TRUE TAP temporal fusion\n",
    "        fused_x0 = self.tap_plugin0(ref_x0, neighbor_x0s)\n",
    "        fused_x1 = self.tap_plugin1(ref_x1, neighbor_x1s)\n",
    "        fused_x2 = self.tap_plugin2(ref_x2, neighbor_x2s)\n",
    "        fused_x3 = self.tap_plugin3(ref_x3, neighbor_x3s)\n",
    "\n",
    "        # Step 5: Continue with decoder\n",
    "        x3_down = self.nafnet.down3(fused_x3)\n",
    "        middle_out = self.nafnet.middle(x3_down)\n",
    "        x3_up = self.nafnet.up3(middle_out)\n",
    "        x3_cat = fused_x3 + x3_up\n",
    "        x3_out = self.nafnet.dec_conv3(x3_cat)\n",
    "\n",
    "        x2_up = self.nafnet.up2(x3_out)\n",
    "        x2_cat = fused_x2 + x2_up\n",
    "        x2_out = self.nafnet.dec_conv2(x2_cat)\n",
    "\n",
    "        x1_up = self.nafnet.up1(x2_out)\n",
    "        x1_cat = fused_x1 + x1_up\n",
    "        x1_out = self.nafnet.dec_conv1(x1_cat)\n",
    "\n",
    "        out = self.nafnet.output_conv(x1_out + fused_x0)\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "# -------------------------\n",
    "# Dataset and Training (Updated for VS Code)\n",
    "# -------------------------\n",
    "class OptimizedTAPDataset(Dataset):\n",
    "    def __init__(self, folder, patch_size=256, n_frames=5, noise_sigma=25, samples_per_video=30, is_train=True):\n",
    "        self.files = sorted(glob.glob(os.path.join(folder, '*.npy')))\n",
    "        if len(self.files) == 0:\n",
    "            raise FileNotFoundError(f\"No .npy files found in {folder}\")\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.n_frames = n_frames\n",
    "        self.half_frames = n_frames // 2\n",
    "        self.noise_sigma = noise_sigma\n",
    "        self.samples_per_video = samples_per_video\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.RandomHorizontalFlip(0.5) if is_train else transforms.Lambda(lambda x: x),\n",
    "        ])\n",
    "\n",
    "        self.video_metadata = []\n",
    "        for f in self.files:\n",
    "            try:\n",
    "                arr = np.load(f)\n",
    "                T = arr.shape[0]\n",
    "                if T > n_frames:  # Only use videos with enough frames\n",
    "                    valid_indices = list(range(self.half_frames, T - self.half_frames))\n",
    "                    self.video_metadata.append((f, T, valid_indices))\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load {f}: {e}\")\n",
    "\n",
    "        print(f\"üìÅ Loaded {len(self.video_metadata)} videos from {folder}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_metadata) * self.samples_per_video\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_idx = idx % len(self.video_metadata)\n",
    "        f, T, valid_indices = self.video_metadata[video_idx]\n",
    "\n",
    "        if self.is_train:\n",
    "            center = random.choice(valid_indices)\n",
    "        else:\n",
    "            center = valid_indices[idx % len(valid_indices)]\n",
    "\n",
    "        arr = np.load(f)\n",
    "        window = arr[center - self.half_frames: center + self.half_frames + 1]\n",
    "        H, W, _ = window[0].shape\n",
    "\n",
    "        if self.is_train:\n",
    "            x = random.randint(0, W - self.patch_size)\n",
    "            y = random.randint(0, H - self.patch_size)\n",
    "            crops = [frame[y:y+self.patch_size, x:x+self.patch_size] for frame in window]\n",
    "        else:\n",
    "            x = (W - self.patch_size) // 2\n",
    "            y = (H - self.patch_size) // 2\n",
    "            crops = [frame[y:y+self.patch_size, x:x+self.patch_size] for frame in window]\n",
    "\n",
    "        crops_rgb = [cv2.cvtColor(crop, cv2.COLOR_BGR2RGB) for crop in crops]\n",
    "        crops_t = [self.transform(crop.astype(np.float32)/255.0) for crop in crops_rgb]\n",
    "\n",
    "        center_clean = crops_t[self.half_frames]\n",
    "\n",
    "        current_sigma = self.noise_sigma\n",
    "        if self.is_train:\n",
    "            current_sigma = random.randint(20, 30)\n",
    "\n",
    "        noisy_crops1 = [c + torch.randn_like(c) * (current_sigma/255.0) for c in crops_t]\n",
    "        noisy_crops2 = [c + torch.randn_like(c) * (current_sigma/255.0) for c in crops_t]\n",
    "\n",
    "        center_noisy1 = noisy_crops1[self.half_frames]\n",
    "        center_noisy2 = noisy_crops2[self.half_frames]\n",
    "\n",
    "        noisy_frames1 = torch.stack(noisy_crops1, dim=0)\n",
    "        noisy_frames2 = torch.stack(noisy_crops2, dim=0)\n",
    "\n",
    "        return noisy_frames1, noisy_frames2, center_clean, center_noisy1, center_noisy2\n",
    "\n",
    "class TrueTAPPipeline:\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"üöÄ Using device: {self.device}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"üéØ GPU: {torch.cuda.get_device_name()}\")\n",
    "            print(f\"üéØ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "        self.criterion = nn.L1Loss()\n",
    "        self.scheduler = None\n",
    "        # Track losses for plotting\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.learning_rates = []\n",
    "\n",
    "    def setup_model(self):\n",
    "        \"\"\"Initialize TRUE TAP model with correct flow\"\"\"\n",
    "        print(\"üîÑ Initializing TRUE TAP model...\")\n",
    "        \n",
    "        self.model = TrueTAPWithNAFNet(\n",
    "            n_frames=self.opt.n_frames,\n",
    "            img_channel=3,\n",
    "            width=self.opt.nafnet_channels,\n",
    "            enc_blks=self.opt.enc_blocks,\n",
    "            dec_blks=self.opt.dec_blocks\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Only optimize TAP temporal plugins\n",
    "        tap_params = []\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if 'tap_plugin' in name or 'deform' in name or 'temporal_att' in name:\n",
    "                tap_params.append(param)\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(tap_params, lr=self.opt.lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=15, gamma=0.5)\n",
    "\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in tap_params)\n",
    "\n",
    "        print(f\"ü§ñ TRUE TAP Model Analysis:\")\n",
    "        print(f\"   Total parameters: {total_params:,}\")\n",
    "        print(f\"   TAP plugins (trainable): {trainable_params:,}\")\n",
    "        print(f\"   NAFNet backbone (frozen): {total_params - trainable_params:,}\")\n",
    "        print(f\"   Plugin ratio: {trainable_params/total_params*100:.1f}%\")\n",
    "        print(f\"   Architecture: Deformable Alignment + Temporal Attention\")\n",
    "\n",
    "        # Test forward pass with memory monitoring\n",
    "        print(\"üß™ Testing TRUE TAP forward pass...\")\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    initial_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "                \n",
    "                test_input = torch.randn(2, self.opt.n_frames, 3, self.opt.patch_size, self.opt.patch_size).to(self.device)\n",
    "                test_output = self.model(test_input)\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    final_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "                    print(f\"‚úÖ TRUE TAP forward pass successful!\")\n",
    "                    print(f\"   Input: {test_input.shape} ‚Üí Output: {test_output.shape}\")\n",
    "                    print(f\"   GPU Memory used: {final_memory - initial_memory:.2f} GB\")\n",
    "                else:\n",
    "                    print(f\"‚úÖ TRUE TAP forward pass successful!\")\n",
    "                    print(f\"   Input: {test_input.shape} ‚Üí Output: {test_output.shape}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Forward pass failed: {e}\")\n",
    "                if \"out of memory\" in str(e):\n",
    "                    print(\"üí° Try reducing batch_size or patch_size\")\n",
    "                raise e\n",
    "\n",
    "    def create_dataloaders(self):\n",
    "        \"\"\"Create dataloaders\"\"\"\n",
    "        print(\"üìä Creating dataloaders...\")\n",
    "\n",
    "        # Check if dataset exists\n",
    "        train_folder = \"./selected_videos/train\"\n",
    "        val_folder = \"./selected_videos/val\"\n",
    "        \n",
    "        if not os.path.exists(train_folder):\n",
    "            raise FileNotFoundError(f\"Training dataset not found at {train_folder}\")\n",
    "        if not os.path.exists(val_folder):\n",
    "            raise FileNotFoundError(f\"Validation dataset not found at {val_folder}\")\n",
    "\n",
    "        train_dataset = OptimizedTAPDataset(\n",
    "            folder=train_folder,\n",
    "            patch_size=self.opt.patch_size,\n",
    "            n_frames=self.opt.n_frames,\n",
    "            noise_sigma=self.opt.noise_sigma,\n",
    "            samples_per_video=30,\n",
    "            is_train=True\n",
    "        )\n",
    "\n",
    "        val_dataset = OptimizedTAPDataset(\n",
    "            folder=val_folder,\n",
    "            patch_size=self.opt.patch_size,\n",
    "            n_frames=self.opt.n_frames,\n",
    "            noise_sigma=self.opt.noise_sigma,\n",
    "            samples_per_video=10,\n",
    "            is_train=False\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.opt.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.opt.num_workers,\n",
    "            pin_memory=True if torch.cuda.is_available() else False,\n",
    "            drop_last=True\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.opt.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.opt.num_workers,\n",
    "            pin_memory=True if torch.cuda.is_available() else False,\n",
    "            drop_last=True\n",
    "        )\n",
    "\n",
    "        print(f\"‚úÖ Train: {len(train_dataset)} samples\")\n",
    "        print(f\"‚úÖ Val: {len(val_dataset)} samples\")\n",
    "\n",
    "        return train_loader, val_loader\n",
    "\n",
    "    def train_epoch(self, train_loader, epoch):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{self.opt.epochs}\",\n",
    "                   bar_format='{l_bar}{bar:20}{r_bar}{bar:-20b}')\n",
    "\n",
    "        for batch_idx, (noisy_frames1, noisy_frames2, center_clean, center_noisy1, center_noisy2) in enumerate(pbar):\n",
    "            noisy_frames1 = noisy_frames1.to(self.device, non_blocking=True)\n",
    "            center_noisy2 = center_noisy2.to(self.device, non_blocking=True)\n",
    "\n",
    "            denoised = self.model(noisy_frames1)\n",
    "            loss = self.criterion(denoised, center_noisy2)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'lr': f'{current_lr:.2e}'\n",
    "            })\n",
    "\n",
    "        self.scheduler.step()\n",
    "        return running_loss / len(train_loader)\n",
    "\n",
    "    def validate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(val_loader, desc=\"Validating\",\n",
    "                   bar_format='{l_bar}{bar:20}{r_bar}{bar:-20b}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for noisy_frames1, noisy_frames2, center_clean, center_noisy1, center_noisy2 in pbar:\n",
    "                noisy_frames1 = noisy_frames1.to(self.device)\n",
    "                center_noisy2 = center_noisy2.to(self.device)\n",
    "\n",
    "                denoised = self.model(noisy_frames1)\n",
    "                loss = self.criterion(denoised, center_noisy2)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                pbar.set_postfix({'val_loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        return val_loss / len(val_loader)\n",
    "\n",
    "    def plot_training_history(self, train_losses, val_losses, show_plot=True):\n",
    "        \"\"\"Enhanced training history plotting with better visualization\"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Create subplots\n",
    "        plt.subplot(2, 1, 1)\n",
    "        \n",
    "        # Plot losses\n",
    "        epochs = range(1, len(train_losses) + 1)\n",
    "        plt.plot(epochs, train_losses, 'b-', linewidth=2, label='Training Loss', alpha=0.8)\n",
    "        plt.plot(epochs, val_losses, 'r-', linewidth=2, label='Validation Loss', alpha=0.8)\n",
    "        \n",
    "        # Find and mark best validation loss\n",
    "        best_epoch = np.argmin(val_losses) + 1\n",
    "        best_val_loss = val_losses[best_epoch - 1]\n",
    "        plt.plot(best_epoch, best_val_loss, 'ro', markersize=8, label=f'Best Val Loss: {best_val_loss:.4f}')\n",
    "        \n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('TRUE TAP Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add annotations\n",
    "        plt.annotate(f'Best Epoch: {best_epoch}\\nVal Loss: {best_val_loss:.4f}', \n",
    "                    xy=(best_epoch, best_val_loss), \n",
    "                    xytext=(best_epoch + 1, best_val_loss + 0.01),\n",
    "                    arrowprops=dict(facecolor='black', shrink=0.05, width=1.5),\n",
    "                    fontsize=10, fontweight='bold')\n",
    "        \n",
    "        # Learning rate subplot\n",
    "        plt.subplot(2, 1, 2)\n",
    "        if hasattr(self, 'learning_rates') and self.learning_rates:\n",
    "            plt.plot(epochs, self.learning_rates, 'g-', linewidth=2, label='Learning Rate')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Learning Rate')\n",
    "            plt.title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.yscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        plot_path = os.path.join(self.opt.save_dir, 'training_history.png')\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        print(f\"üìä Training history plot saved: {plot_path}\")\n",
    "        \n",
    "        if show_plot:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "\n",
    "    def save_training_data(self):\n",
    "        \"\"\"Save training data to file for later analysis\"\"\"\n",
    "        training_data = {\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses,\n",
    "            'learning_rates': self.learning_rates,\n",
    "            'config': vars(self.opt)\n",
    "        }\n",
    "        \n",
    "        data_path = os.path.join(self.opt.save_dir, 'training_data.npy')\n",
    "        np.save(data_path, training_data, allow_pickle=True)\n",
    "        print(f\"üíæ Training data saved: {data_path}\")\n",
    "\n",
    "    def train(self):\n",
    "        train_loader, val_loader = self.create_dataloaders()\n",
    "\n",
    "        print(f\"üöÄ Starting TRUE TAP training for {self.opt.epochs} epochs...\")\n",
    "        print(\"üí° Only TAP temporal plugins are being trained\")\n",
    "        print(\"üéØ Flow: Noisy ‚Üí Pre-trained Denoiser ‚Üí TAP ‚Üí Denoised\")\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        early_stop_patience = 10\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(self.opt.epochs):\n",
    "            epoch_start = time.time()\n",
    "            \n",
    "            train_loss = self.train_epoch(train_loader, epoch)\n",
    "            val_loss = self.validate(val_loader)\n",
    "            \n",
    "            epoch_time = time.time() - epoch_start\n",
    "            \n",
    "            # Track metrics\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.learning_rates.append(self.optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            print(f\"üìà Epoch {epoch+1}/{self.opt.epochs}: \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Time: {epoch_time:.1f}s\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                self.save_checkpoint(epoch, is_best=True)\n",
    "                print(f\"üèÜ New best model! Val Loss: {val_loss:.4f}\")\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= early_stop_patience:\n",
    "                    print(f\"‚èπÔ∏è Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "            # Save checkpoint and plot periodically\n",
    "            if (epoch + 1) % self.opt.eval_step == 0:\n",
    "                self.save_checkpoint(epoch)\n",
    "                print(f\"üíæ Saved TAP checkpoint at epoch {epoch+1}\")\n",
    "                \n",
    "                # Plot intermediate progress\n",
    "                self.plot_training_history(self.train_losses, self.val_losses, show_plot=False)\n",
    "\n",
    "        # Final plots and saves\n",
    "        self.plot_training_history(self.train_losses, self.val_losses, show_plot=True)\n",
    "        self.save_training_data()\n",
    "        self.save_checkpoint(self.opt.epochs - 1, is_final=True)\n",
    "        \n",
    "        print(\"üéâ TRUE TAP training completed!\")\n",
    "        print(f\"üìÅ Final TAP weights saved as: {self.opt.save_dir}/true_tap_final.pth\")\n",
    "        print(f\"üèÜ Best validation loss: {best_val_loss:.4f}\")\n",
    "        print(f\"üìä Training curves saved as: {self.opt.save_dir}/training_history.png\")\n",
    "\n",
    "    def save_checkpoint(self, epoch, is_final=False, is_best=False):\n",
    "        if is_final:\n",
    "            filename = 'true_tap_final.pth'\n",
    "        elif is_best:\n",
    "            filename = 'true_tap_best.pth'\n",
    "        else:\n",
    "            filename = f'true_tap_epoch_{epoch+1}.pth'\n",
    "\n",
    "        # OPTION 1: Save ONLY TAP weights (current behavior)\n",
    "        trainable_state_dict = {}\n",
    "        for k, v in self.model.state_dict().items():\n",
    "            if any(keyword in k for keyword in ['tap_plugin', 'deform', 'temporal_att']):\n",
    "                trainable_state_dict[k] = v\n",
    "\n",
    "        # OPTION 2: Save COMPLETE model (recommended for testing)\n",
    "        complete_state_dict = self.model.state_dict()\n",
    "\n",
    "        # Save both for compatibility\n",
    "        save_dict = {\n",
    "            'epoch': epoch,\n",
    "            'tap_weights': trainable_state_dict,  # Only TAP plugins\n",
    "            'complete_model': complete_state_dict,  # Full model\n",
    "            'train_loss': self.train_losses[-1] if self.train_losses else None,\n",
    "            'val_loss': self.val_losses[-1] if self.val_losses else None,\n",
    "            'best_val_loss': min(self.val_losses) if self.val_losses else None\n",
    "        }\n",
    "\n",
    "        torch.save(save_dict, os.path.join(self.opt.save_dir, filename))\n",
    "        \n",
    "        if is_final or is_best:\n",
    "            print(f\"üíæ Saved model to {filename}\")\n",
    "            print(f\"   - TAP parameters: {len(trainable_state_dict)}\")\n",
    "            print(f\"   - Total parameters: {len(complete_state_dict)}\")\n",
    "            if is_best:\n",
    "                print(f\"   - Best validation loss: {save_dict['best_val_loss']:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Main Execution\n",
    "# -------------------------\n",
    "def main():\n",
    "    # Check GPU availability\n",
    "    if os.name == 'nt':\n",
    "        import multiprocessing\n",
    "        multiprocessing.freeze_support()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"üéØ GPU is available! Training will use CUDA.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  GPU not available. Training will use CPU (slow).\")\n",
    "    \n",
    "    opt = TAPOptions()\n",
    "    opt.print_options()\n",
    "\n",
    "    # Check if dataset exists\n",
    "    if not os.path.exists(\"./selected_videos/train\"):\n",
    "        print(\"‚ùå DAVIS dataset not found! Please run dataset preparation first.\")\n",
    "        print(\"üí° Run the DAVIS dataset preparation script first.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        pipeline = TrueTAPPipeline(opt)\n",
    "        pipeline.setup_model()\n",
    "        pipeline.train()\n",
    "\n",
    "        print(\"‚úÖ TRUE TAP training completed successfully!\")\n",
    "        print(\"üéØ Correct Flow Implemented: Noisy ‚Üí Pre-trained Denoiser ‚Üí TAP ‚Üí Denoised\")\n",
    "        \n",
    "        # Final summary\n",
    "        if pipeline.train_losses and pipeline.val_losses:\n",
    "            final_train_loss = pipeline.train_losses[-1]\n",
    "            final_val_loss = pipeline.val_losses[-1]\n",
    "            best_val_loss = min(pipeline.val_losses)\n",
    "            improvement = pipeline.train_losses[0] - final_train_loss\n",
    "            \n",
    "            print(f\"\\nüìä FINAL TRAINING SUMMARY:\")\n",
    "            print(f\"   Initial Train Loss: {pipeline.train_losses[0]:.4f}\")\n",
    "            print(f\"   Final Train Loss: {final_train_loss:.4f}\")\n",
    "            print(f\"   Final Val Loss: {final_val_loss:.4f}\")\n",
    "            print(f\"   Best Val Loss: {best_val_loss:.4f}\")\n",
    "            print(f\"   Total Improvement: {improvement:.4f}\")\n",
    "            print(f\"   Overfitting: {final_val_loss - final_train_loss:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set seeds for reproducibility\n",
    "    random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(42)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True  # Can improve performance\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ead8fed",
   "metadata": {},
   "source": [
    "TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9444625b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages for Enhanced TAP + UDVD...\n",
      "==================================================\n",
      "‚úÖ torch is already installed\n",
      "‚úÖ torchvision is already installed\n",
      "‚úÖ numpy is already installed\n",
      "üì¶ Installing opencv-python...\n",
      "‚úÖ opencv-python installed successfully\n",
      "üì¶ Installing Pillow...\n",
      "‚úÖ Pillow installed successfully\n",
      "‚úÖ tqdm is already installed\n",
      "‚úÖ matplotlib is already installed\n",
      "üì¶ Installing scikit-image...\n",
      "‚úÖ scikit-image installed successfully\n",
      "‚úÖ scipy is already installed\n",
      "‚úÖ pathlib is already installed\n",
      "\n",
      "==================================================\n",
      "‚úÖ All packages installed successfully!\n",
      "You can now run the training and testing code.\n"
     ]
    }
   ],
   "source": [
    "# install_requirements.py\n",
    "import subprocess\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package if not already installed\"\"\"\n",
    "    try:\n",
    "        importlib.import_module(package)\n",
    "        print(f\"‚úÖ {package} is already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"üì¶ Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"‚úÖ {package} installed successfully\")\n",
    "\n",
    "def main():\n",
    "    print(\"Installing required packages for Enhanced TAP + UDVD...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Core packages\n",
    "    packages = [\n",
    "        \"torch\",\n",
    "        \"torchvision\", \n",
    "        \"numpy\",\n",
    "        \"opencv-python\",\n",
    "        \"Pillow\",\n",
    "        \"tqdm\",\n",
    "        \"matplotlib\",\n",
    "        \"scikit-image\",\n",
    "        \"scipy\",\n",
    "        \"pathlib\"\n",
    "    ]\n",
    "    \n",
    "    for package in packages:\n",
    "        install_package(package)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"‚úÖ All packages installed successfully!\")\n",
    "    print(\"You can now run the training and testing code.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dd5ee61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ TRUE TAP MODEL TESTING - VS CODE\n",
      "======================================================================\n",
      "\n",
      "üéØ FLOW:\n",
      "   Noisy -> Pre-trained Denoiser -> TAP Temporal Fusion -> Denoised\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Using test data from: ./selected_videos/test\n",
      "üîß Testing on device: cuda\n",
      "======================================================================\n",
      "üéØ Initializing TRUE TAP Model\n",
      "======================================================================\n",
      "üìÇ Loading weights from: ./output_models/true_tap_final.pth\n",
      "‚úÖ Successfully loaded 0 TAP parameters\n",
      "\n",
      "üß™ Testing model forward pass...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAB\\AppData\\Local\\Temp\\ipykernel_11096\\3609234219.py:566: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Forward pass successful! torch.Size([1, 5, 3, 64, 64]) ‚Üí torch.Size([1, 3, 64, 64])\n",
      "üìÅ Found 8 .npy files for testing\n",
      "üìÅ Found 8 sequences for testing\n",
      "üîß DataLoader settings: workers=0, pin_memory=False\n",
      "\n",
      "üöÄ Testing TAP on 40 sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üé¨ Testing TAP: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:07<00:00,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä TRUE TAP TEST RESULTS\n",
      "======================================================================\n",
      "\n",
      "üìà OVERALL PERFORMANCE:\n",
      "   Noisy PSNR:    18.90 dB\n",
      "   Denoised PSNR: 12.81 dB\n",
      "   Improvement:   +-6.08 dB\n",
      "   Noisy SSIM:    0.3437\n",
      "   Denoised SSIM: 0.2952\n",
      "\n",
      "üìÑ Results saved to: ./test_results\\test_results.txt\n",
      "üñºÔ∏è  Visualizations saved in: ./test_results\\visualizations\n",
      "\n",
      "‚úÖ Testing completed!\n",
      "üìÅ Results saved in: ./test_results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TAP TESTING CODE - VS CODE VERSION\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import random\n",
    "\n",
    "# Windows multiprocessing fix\n",
    "if os.name == 'nt':\n",
    "    import multiprocessing\n",
    "    multiprocessing.freeze_support()\n",
    "\n",
    "# -------------------------\n",
    "# Configuration for Testing (UPDATED PATHS)\n",
    "# -------------------------\n",
    "class TestConfig:\n",
    "    def __init__(self):\n",
    "        self.model_path = './output_models/true_tap_final.pth'  # Updated path\n",
    "        self.test_data_path = './selected_videos/test'  # Updated path\n",
    "        self.davis_root = './davis_data/DAVIS'  # Updated path\n",
    "        self.output_dir = './test_results'  # Updated path\n",
    "        self.n_frames = 5\n",
    "        self.noise_sigma = 30\n",
    "        self.patch_size = 256\n",
    "        self.batch_size = 1\n",
    "        self.num_display_samples = 6\n",
    "        self.save_visualizations = True\n",
    "        self.display_images = True\n",
    "        self.nafnet_channels = 48\n",
    "        self.enc_blocks = [2, 3, 3]\n",
    "        self.dec_blocks = [3, 3, 2]\n",
    "        self.num_workers = 0  # ‚Üê CRITICAL FOR WINDOWS\n",
    "\n",
    "# -------------------------\n",
    "# Model Components (EXACTLY as in training code)\n",
    "# -------------------------\n",
    "\n",
    "class SimpleChannelAttention(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        bottleneck = max(channel // 8, 4)\n",
    "        self.ca = nn.Sequential(\n",
    "            nn.Conv2d(channel, bottleneck, 1, padding=0, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(bottleneck, channel, 1, padding=0, bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)\n",
    "        y = self.ca(y)\n",
    "        return x * y\n",
    "\n",
    "class SimpleNAFBlock(nn.Module):\n",
    "    def __init__(self, c, DW_Expand=2):\n",
    "        super().__init__()\n",
    "        dw_channel = c * DW_Expand\n",
    "        self.conv1 = nn.Conv2d(c, dw_channel, 1, padding=0, bias=True)\n",
    "        self.conv2 = nn.Conv2d(dw_channel, dw_channel, 3, padding=1, groups=dw_channel, bias=True)\n",
    "        self.conv3 = nn.Conv2d(dw_channel, c, 1, padding=0, bias=True)\n",
    "        self.sca = SimpleChannelAttention(dw_channel)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = self.conv1(inp)\n",
    "        x = self.act(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.sca(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv3(x)\n",
    "        return x + inp\n",
    "\n",
    "class PretrainedNAFNet(nn.Module):\n",
    "    def __init__(self, img_channel=3, width=48, enc_blks=[2, 3, 3], dec_blks=[3, 3, 2]):\n",
    "        super().__init__()\n",
    "        self.img_channel = img_channel\n",
    "        self.width = width\n",
    "\n",
    "        self.enc_conv0 = nn.Conv2d(img_channel, width, 3, padding=1, bias=True)\n",
    "        self.enc_conv1 = nn.Sequential(*[SimpleNAFBlock(width) for _ in range(enc_blks[0])])\n",
    "        self.down1 = nn.Conv2d(width, width*2, 2, 2)\n",
    "        self.enc_conv2 = nn.Sequential(*[SimpleNAFBlock(width*2) for _ in range(enc_blks[1])])\n",
    "        self.down2 = nn.Conv2d(width*2, width*4, 2, 2)\n",
    "        self.enc_conv3 = nn.Sequential(*[SimpleNAFBlock(width*4) for _ in range(enc_blks[2])])\n",
    "        self.down3 = nn.Conv2d(width*4, width*8, 2, 2)\n",
    "        self.middle = nn.Sequential(*[SimpleNAFBlock(width*8) for _ in range(2)])\n",
    "        self.up3 = nn.Sequential(\n",
    "            nn.Conv2d(width*8, width*4*4, 1, bias=False),\n",
    "            nn.PixelShuffle(2)\n",
    "        )\n",
    "        self.dec_conv3 = nn.Sequential(*[SimpleNAFBlock(width*4) for _ in range(dec_blks[0])])\n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.Conv2d(width*4, width*2*4, 1, bias=False),\n",
    "            nn.PixelShuffle(2)\n",
    "        )\n",
    "        self.dec_conv2 = nn.Sequential(*[SimpleNAFBlock(width*2) for _ in range(dec_blks[1])])\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.Conv2d(width*2, width*4, 1, bias=False),\n",
    "            nn.PixelShuffle(2)\n",
    "        )\n",
    "        self.dec_conv1 = nn.Sequential(*[SimpleNAFBlock(width) for _ in range(dec_blks[2])])\n",
    "        self.output_conv = nn.Conv2d(width, img_channel, 3, padding=1, bias=True)\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.enc_conv0(x)\n",
    "        x1 = self.enc_conv1(x0)\n",
    "        x1_down = self.down1(x1)\n",
    "        x2 = self.enc_conv2(x1_down)\n",
    "        x2_down = self.down2(x2)\n",
    "        x3 = self.enc_conv3(x2_down)\n",
    "        x3_down = self.down3(x3)\n",
    "        middle_out = self.middle(x3_down)\n",
    "        x3_up = self.up3(middle_out)\n",
    "        x3_cat = x3 + x3_up\n",
    "        x3_out = self.dec_conv3(x3_cat)\n",
    "        x2_up = self.up2(x3_out)\n",
    "        x2_cat = x2 + x2_up\n",
    "        x2_out = self.dec_conv2(x2_cat)\n",
    "        x1_up = self.up1(x2_out)\n",
    "        x1_cat = x1 + x1_up\n",
    "        x1_out = self.dec_conv1(x1_cat)\n",
    "        out = self.output_conv(x1_out + x0)\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "    def get_skip_features(self, x):\n",
    "        with torch.no_grad():\n",
    "            x0 = self.enc_conv0(x)\n",
    "            x1 = self.enc_conv1(x0)\n",
    "            x1_down = self.down1(x1)\n",
    "            x2 = self.enc_conv2(x1_down)\n",
    "            x2_down = self.down2(x2)\n",
    "            x3 = self.enc_conv3(x2_down)\n",
    "        return x0, x1, x2, x3\n",
    "\n",
    "class DeformableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.offset_conv = nn.Conv2d(in_channels * 2, 2 * kernel_size * kernel_size,\n",
    "                                   kernel_size=3, padding=1)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "                            padding=padding, bias=True)\n",
    "        self.offset_conv.weight.data.zero_()\n",
    "        self.offset_conv.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, ref_feat):\n",
    "        offset_input = torch.cat([ref_feat, x], dim=1)\n",
    "        offsets = self.offset_conv(offset_input)\n",
    "        aligned = self.deform_conv2d(x, offsets, self.conv)\n",
    "        return aligned\n",
    "\n",
    "    def deform_conv2d(self, x, offsets, weight_conv):\n",
    "        B, C, H, W = x.shape\n",
    "        kh, kw = self.kernel_size, self.kernel_size\n",
    "\n",
    "        y_coords, x_coords = torch.meshgrid(\n",
    "            torch.arange(H, device=x.device, dtype=torch.float32),\n",
    "            torch.arange(W, device=x.device, dtype=torch.float32),\n",
    "            indexing='ij'\n",
    "        )\n",
    "        y_coords = 2.0 * y_coords / (H - 1) - 1.0\n",
    "        x_coords = 2.0 * x_coords / (W - 1) - 1.0\n",
    "        grid = torch.stack([x_coords, y_coords], dim=-1).unsqueeze(0).repeat(B, 1, 1, 1)\n",
    "\n",
    "        offsets = offsets.permute(0, 2, 3, 1).reshape(B, H, W, kh * kw, 2) * 0.1\n",
    "\n",
    "        sampling_grids = []\n",
    "        for i in range(kh):\n",
    "            for j in range(kw):\n",
    "                ky = (i - kh // 2) * 2.0 / H\n",
    "                kx = (j - kw // 2) * 2.0 / W\n",
    "                kernel_offset = torch.tensor([kx, ky], device=x.device).view(1, 1, 1, 2)\n",
    "                idx = i * kw + j\n",
    "                total_offset = grid + kernel_offset + offsets[:, :, :, idx, :]\n",
    "                sampling_grids.append(total_offset)\n",
    "\n",
    "        sampling_grid = torch.stack(sampling_grids, dim=3).reshape(B, H, W * kh * kw, 2)\n",
    "        x_sampled = F.grid_sample(x, sampling_grid, mode='bilinear',\n",
    "                                   padding_mode='zeros', align_corners=False)\n",
    "        x_sampled = x_sampled.view(B, C, H, W, kh * kw).permute(0, 1, 4, 2, 3)\n",
    "        x_sampled = x_sampled.reshape(B, C, kh * kw, H * W)\n",
    "\n",
    "        weight = weight_conv.weight.view(weight_conv.out_channels, C, kh * kw)\n",
    "        output = torch.einsum('ock,bckn->bon', weight, x_sampled)\n",
    "        output = output.view(B, weight_conv.out_channels, H, W)\n",
    "\n",
    "        if weight_conv.bias is not None:\n",
    "            output += weight_conv.bias.view(1, -1, 1, 1)\n",
    "        return output\n",
    "\n",
    "class DeformableAlignment(nn.Module):\n",
    "    def __init__(self, channels, n_frames=5):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.n_frames = n_frames\n",
    "        self.deform_conv = DeformableConv2d(channels, channels)\n",
    "        self.align_feat_extract = nn.Sequential(\n",
    "            nn.Conv2d(channels * 2, channels, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, ref_feat, neighbor_feat):\n",
    "        aligned_feat = self.deform_conv(neighbor_feat, ref_feat)\n",
    "        aligned_feat = aligned_feat + neighbor_feat\n",
    "        return aligned_feat\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, channels, n_frames=5):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.n_frames = n_frames\n",
    "        self.temporal_att = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels // 4, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels // 4, channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.motion_conv = nn.Sequential(\n",
    "            nn.Conv2d(channels * 2, channels, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, aligned_feats):\n",
    "        B, T, C, H, W = aligned_feats.shape\n",
    "        ref_feat = aligned_feats[:, T//2]\n",
    "        motion_features = []\n",
    "\n",
    "        for t in range(T):\n",
    "            if t != T//2:\n",
    "                motion = torch.cat([ref_feat, aligned_feats[:, t]], dim=1)\n",
    "                motion_feat = self.motion_conv(motion)\n",
    "                motion_features.append(motion_feat.unsqueeze(1))\n",
    "\n",
    "        motion_features = torch.cat(motion_features, dim=1)\n",
    "        temporal_weights = self.temporal_att(\n",
    "            motion_features.view(B * (T-1), C, H, W)\n",
    "        ).view(B, T-1, C, 1, 1)\n",
    "\n",
    "        weighted_features = []\n",
    "        idx = 0\n",
    "        for t in range(T):\n",
    "            if t == T//2:\n",
    "                weighted_features.append(aligned_feats[:, t].unsqueeze(1))\n",
    "            else:\n",
    "                weighted_feat = aligned_feats[:, t] * temporal_weights[:, idx]\n",
    "                weighted_features.append(weighted_feat.unsqueeze(1))\n",
    "                idx += 1\n",
    "\n",
    "        return torch.cat(weighted_features, dim=1)\n",
    "\n",
    "class TAPTemporalPlugin(nn.Module):\n",
    "    def __init__(self, channels, n_frames=5):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.n_frames = n_frames\n",
    "        self.half_frames = n_frames // 2\n",
    "        self.deform_align = DeformableAlignment(channels, n_frames)\n",
    "        self.temporal_att = TemporalAttention(channels, n_frames)\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(channels * n_frames, channels * 2, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels * 2, channels, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, ref_feat, neighbor_feats):\n",
    "        B, C, H, W = ref_feat.shape\n",
    "        T = len(neighbor_feats) + 1\n",
    "\n",
    "        aligned_feats = [ref_feat.unsqueeze(1)]\n",
    "        for neighbor_feat in neighbor_feats:\n",
    "            aligned_neighbor = self.deform_align(ref_feat, neighbor_feat)\n",
    "            aligned_feats.append(aligned_neighbor.unsqueeze(1))\n",
    "\n",
    "        aligned_feats = torch.cat(aligned_feats, dim=1)\n",
    "        attended_feats = self.temporal_att(aligned_feats)\n",
    "        fused_feats = attended_feats.view(B, T * C, H, W)\n",
    "        output = self.fusion(fused_feats) + ref_feat\n",
    "        return output\n",
    "\n",
    "class TrueTAPWithNAFNet(nn.Module):\n",
    "    def __init__(self, n_frames=5, img_channel=3, width=48, enc_blks=[2, 3, 3], dec_blks=[3, 3, 2]):\n",
    "        super().__init__()\n",
    "        self.n_frames = n_frames\n",
    "        self.half_frames = n_frames // 2\n",
    "        self.nafnet = PretrainedNAFNet(img_channel, width, enc_blks, dec_blks)\n",
    "        self.tap_plugin0 = TAPTemporalPlugin(width, n_frames)\n",
    "        self.tap_plugin1 = TAPTemporalPlugin(width, n_frames)\n",
    "        self.tap_plugin2 = TAPTemporalPlugin(width*2, n_frames)\n",
    "        self.tap_plugin3 = TAPTemporalPlugin(width*4, n_frames)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        B, T, C, H, W = inp.shape\n",
    "        if T != self.n_frames:\n",
    "            raise ValueError(f\"Expected {self.n_frames} frames, got {T}\")\n",
    "\n",
    "        # Apply pre-trained denoiser to each frame\n",
    "        denoised_frames = []\n",
    "        for t in range(T):\n",
    "            frame_denoised = self.nafnet(inp[:, t])\n",
    "            denoised_frames.append(frame_denoised)\n",
    "        denoised_sequence = torch.stack(denoised_frames, dim=1)\n",
    "\n",
    "        # Extract reference and neighbors\n",
    "        ref_frame = denoised_sequence[:, T//2]\n",
    "        neighbor_frames = [denoised_sequence[:, i] for i in range(T) if i != T//2]\n",
    "\n",
    "        # Extract skip features\n",
    "        ref_x0, ref_x1, ref_x2, ref_x3 = self.nafnet.get_skip_features(ref_frame)\n",
    "        neighbor_x0s, neighbor_x1s, neighbor_x2s, neighbor_x3s = [], [], [], []\n",
    "        for frame in neighbor_frames:\n",
    "            n_x0, n_x1, n_x2, n_x3 = self.nafnet.get_skip_features(frame)\n",
    "            neighbor_x0s.append(n_x0)\n",
    "            neighbor_x1s.append(n_x1)\n",
    "            neighbor_x2s.append(n_x2)\n",
    "            neighbor_x3s.append(n_x3)\n",
    "\n",
    "        # Apply TAP fusion\n",
    "        fused_x0 = self.tap_plugin0(ref_x0, neighbor_x0s)\n",
    "        fused_x1 = self.tap_plugin1(ref_x1, neighbor_x1s)\n",
    "        fused_x2 = self.tap_plugin2(ref_x2, neighbor_x2s)\n",
    "        fused_x3 = self.tap_plugin3(ref_x3, neighbor_x3s)\n",
    "\n",
    "        # Continue with decoder\n",
    "        x3_down = self.nafnet.down3(fused_x3)\n",
    "        middle_out = self.nafnet.middle(x3_down)\n",
    "        x3_up = self.nafnet.up3(middle_out)\n",
    "        x3_cat = fused_x3 + x3_up\n",
    "        x3_out = self.nafnet.dec_conv3(x3_cat)\n",
    "\n",
    "        x2_up = self.nafnet.up2(x3_out)\n",
    "        x2_cat = fused_x2 + x2_up\n",
    "        x2_out = self.nafnet.dec_conv2(x2_cat)\n",
    "\n",
    "        x1_up = self.nafnet.up1(x2_out)\n",
    "        x1_cat = fused_x1 + x1_up\n",
    "        x1_out = self.nafnet.dec_conv1(x1_cat)\n",
    "\n",
    "        out = self.nafnet.output_conv(x1_out + fused_x0)\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "# -------------------------\n",
    "# Test Dataset (UPDATED FOR WINDOWS)\n",
    "# -------------------------\n",
    "class TAPTestDataset(Dataset):\n",
    "    def __init__(self, folder, davis_root=None, n_frames=5, noise_sigma=30, patch_size=256):\n",
    "        self.n_frames = n_frames\n",
    "        self.noise_sigma = noise_sigma / 255.0\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.sequences = []\n",
    "        \n",
    "        # Check for .npy files first (processed videos)\n",
    "        if os.path.exists(folder):\n",
    "            npy_files = glob.glob(os.path.join(folder, \"*.npy\"))\n",
    "            if npy_files:\n",
    "                print(f\"üìÅ Found {len(npy_files)} .npy files for testing\")\n",
    "                for npy_file in npy_files:\n",
    "                    video_name = os.path.splitext(os.path.basename(npy_file))[0]\n",
    "                    self.sequences.append((video_name, npy_file))\n",
    "        \n",
    "        # If no .npy files, check for DAVIS dataset\n",
    "        if not self.sequences and davis_root and os.path.exists(davis_root):\n",
    "            jpeg_dir = os.path.join(davis_root, 'JPEGImages', '480p')\n",
    "            if os.path.exists(jpeg_dir):\n",
    "                for video_name in os.listdir(jpeg_dir):\n",
    "                    video_path = os.path.join(jpeg_dir, video_name)\n",
    "                    if os.path.isdir(video_path):\n",
    "                        frames = sorted(glob.glob(os.path.join(video_path, '*.jpg')))\n",
    "                        if len(frames) >= n_frames:\n",
    "                            self.sequences.append((video_name, frames))\n",
    "\n",
    "        if not self.sequences:\n",
    "            print(\"‚ö†Ô∏è  No video sequences found. Creating synthetic test data...\")\n",
    "            self.sequences = self._create_synthetic_sequences()\n",
    "\n",
    "        print(f\"üìÅ Found {len(self.sequences)} sequences for testing\")\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def _create_synthetic_sequences(self):\n",
    "        sequences = []\n",
    "        for i in range(3):\n",
    "            frames = [f\"synthetic_frame_{j}\" for j in range(20)]\n",
    "            sequences.append((f'synthetic_{i+1}', frames))\n",
    "        return sequences\n",
    "\n",
    "    def _load_synthetic_frame(self, idx, video_name):\n",
    "        h, w = 480, 640\n",
    "        frame = np.ones((h, w, 3), dtype=np.float32) * 0.5\n",
    "        t = idx * 0.1\n",
    "        center_x = int(w/2 + w/4 * np.sin(t))\n",
    "        center_y = int(h/2 + h/4 * np.cos(t))\n",
    "        cv2.circle(frame, (center_x, center_y), 50, (0.8, 0.6, 0.4), -1)\n",
    "        cv2.rectangle(frame, (100, 100), (200, 200), (0.4, 0.8, 0.6), -1)\n",
    "        return frame\n",
    "\n",
    "    def _load_npy_video(self, npy_path, start_idx):\n",
    "        \"\"\"Load frames from .npy file\"\"\"\n",
    "        try:\n",
    "            video_data = np.load(npy_path)\n",
    "            T, H, W, C = video_data.shape\n",
    "            \n",
    "            # Ensure we have enough frames\n",
    "            if T < self.n_frames:\n",
    "                # Repeat frames if video is too short\n",
    "                repeated_data = np.repeat(video_data, (self.n_frames + T - 1) // T, axis=0)\n",
    "                video_data = repeated_data[:self.n_frames]\n",
    "                T = self.n_frames\n",
    "            \n",
    "            frames = []\n",
    "            for i in range(self.n_frames):\n",
    "                frame_idx = (start_idx + i) % T\n",
    "                frame = video_data[frame_idx].astype(np.float32) / 255.0\n",
    "                \n",
    "                # Resize if needed\n",
    "                if frame.shape[0] != self.patch_size or frame.shape[1] != self.patch_size:\n",
    "                    frame = cv2.resize(frame, (self.patch_size, self.patch_size))\n",
    "                \n",
    "                frames.append(frame)\n",
    "            \n",
    "            return frames\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {npy_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.sequences) * 5, 50)  # Limit for testing\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq_idx = idx % len(self.sequences)\n",
    "        video_name, data_path = self.sequences[seq_idx]\n",
    "\n",
    "        # Determine start index\n",
    "        if isinstance(data_path, str) and data_path.endswith('.npy'):\n",
    "            # Load from .npy file\n",
    "            max_start = 10  # Limit for .npy files\n",
    "            start_idx = np.random.randint(0, max_start)\n",
    "            clean_frames = self._load_npy_video(data_path, start_idx)\n",
    "        else:\n",
    "            # Load from image files\n",
    "            frames = data_path\n",
    "            if len(frames) > self.n_frames:\n",
    "                start_idx = np.random.randint(0, len(frames) - self.n_frames)\n",
    "            else:\n",
    "                start_idx = 0\n",
    "\n",
    "            clean_frames = []\n",
    "            for i in range(self.n_frames):\n",
    "                frame_idx = start_idx + i\n",
    "                if frame_idx < len(frames) and os.path.exists(frames[frame_idx]):\n",
    "                    frame = cv2.imread(frames[frame_idx])\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    frame = frame.astype(np.float32) / 255.0\n",
    "                else:\n",
    "                    frame = self._load_synthetic_frame(frame_idx, video_name)\n",
    "\n",
    "                if frame.shape[0] != self.patch_size or frame.shape[1] != self.patch_size:\n",
    "                    frame = cv2.resize(frame, (self.patch_size, self.patch_size))\n",
    "                clean_frames.append(frame)\n",
    "\n",
    "        # Convert to tensors and add noise\n",
    "        clean_tensors = [self.transform(frame) for frame in clean_frames]\n",
    "        clean_sequence = torch.stack(clean_tensors)\n",
    "        noise = torch.randn_like(clean_sequence) * self.noise_sigma\n",
    "        noisy_sequence = torch.clamp(clean_sequence + noise, 0, 1)\n",
    "        center_idx = self.n_frames // 2\n",
    "\n",
    "        return {\n",
    "            'noisy_frames': noisy_sequence,\n",
    "            'clean_frames': clean_sequence,\n",
    "            'center_clean': clean_sequence[center_idx],\n",
    "            'center_noisy': noisy_sequence[center_idx],\n",
    "            'video_name': video_name,\n",
    "            'frame_idx': start_idx + center_idx\n",
    "        }\n",
    "\n",
    "# -------------------------\n",
    "# Visualization & Testing\n",
    "# -------------------------\n",
    "def visualize_results(noisy, denoised, clean, save_path, video_name, frame_idx, display=False):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    noisy_np = np.clip(noisy.permute(1, 2, 0).cpu().numpy(), 0, 1)\n",
    "    denoised_np = np.clip(denoised.permute(1, 2, 0).cpu().numpy(), 0, 1)\n",
    "    clean_np = np.clip(clean.permute(1, 2, 0).cpu().numpy(), 0, 1)\n",
    "\n",
    "    psnr_noisy = compare_psnr(clean_np, noisy_np, data_range=1.0)\n",
    "    psnr_denoised = compare_psnr(clean_np, denoised_np, data_range=1.0)\n",
    "    ssim_noisy = compare_ssim(clean_np, noisy_np, channel_axis=2, data_range=1.0)\n",
    "    ssim_denoised = compare_ssim(clean_np, denoised_np, channel_axis=2, data_range=1.0)\n",
    "\n",
    "    axes[0].imshow(noisy_np)\n",
    "    axes[0].set_title(f'Noisy Input\\nPSNR: {psnr_noisy:.2f}dB, SSIM: {ssim_noisy:.3f}')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(denoised_np)\n",
    "    axes[1].set_title(f'TAP Denoised\\nPSNR: {psnr_denoised:.2f}dB, SSIM: {ssim_denoised:.3f}')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    axes[2].imshow(clean_np)\n",
    "    axes[2].set_title('Clean Ground Truth')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    plt.suptitle(f'{video_name} - Frame {frame_idx}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=100, bbox_inches='tight')\n",
    "    if display:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "class TAPTester:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"üîß Testing on device: {self.device}\")\n",
    "        self.model = None\n",
    "        \n",
    "        # Create output directories\n",
    "        os.makedirs(config.output_dir, exist_ok=True)\n",
    "        self.viz_dir = os.path.join(config.output_dir, 'visualizations')\n",
    "        os.makedirs(self.viz_dir, exist_ok=True)\n",
    "\n",
    "    def setup_model(self):\n",
    "        print(\"=\"*70)\n",
    "        print(\"üéØ Initializing TRUE TAP Model\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        self.model = TrueTAPWithNAFNet(\n",
    "            n_frames=self.config.n_frames,\n",
    "            img_channel=3,\n",
    "            width=self.config.nafnet_channels,\n",
    "            enc_blks=self.config.enc_blocks,\n",
    "            dec_blks=self.config.dec_blocks\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Try multiple possible model paths\n",
    "        possible_model_paths = [\n",
    "            self.config.model_path,\n",
    "            './output_models/true_tap_best.pth',\n",
    "            './output_models/true_tap_epoch_30.pth',\n",
    "            './output_models/true_tap_epoch_25.pth',\n",
    "            './output_models/true_tap_epoch_20.pth',\n",
    "        ]\n",
    "\n",
    "        model_loaded = False\n",
    "        for model_path in possible_model_paths:\n",
    "            if os.path.exists(model_path):\n",
    "                print(f\"üìÇ Loading weights from: {model_path}\")\n",
    "                try:\n",
    "                    checkpoint = torch.load(model_path, map_location=self.device)\n",
    "\n",
    "                    # Load only TAP plugin weights\n",
    "                    model_state_dict = self.model.state_dict()\n",
    "                    loaded_count = 0\n",
    "\n",
    "                    for k, v in checkpoint.items():\n",
    "                        if k in model_state_dict and v.shape == model_state_dict[k].shape:\n",
    "                            model_state_dict[k] = v\n",
    "                            loaded_count += 1\n",
    "\n",
    "                    self.model.load_state_dict(model_state_dict, strict=False)\n",
    "                    print(f\"‚úÖ Successfully loaded {loaded_count} TAP parameters\")\n",
    "                    model_loaded = True\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error loading {model_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        if not model_loaded:\n",
    "            print(\"‚ùå No valid model weights found!\")\n",
    "            return False\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        # Test forward pass\n",
    "        print(\"\\nüß™ Testing model forward pass...\")\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                test_input = torch.randn(1, self.config.n_frames, 3, 64, 64).to(self.device)\n",
    "                test_output = self.model(test_input)\n",
    "                print(f\"‚úÖ Forward pass successful! {test_input.shape} ‚Üí {test_output.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Forward pass failed: {e}\")\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def test_model(self):\n",
    "        if not self.setup_model():\n",
    "            return\n",
    "\n",
    "        test_dataset = TAPTestDataset(\n",
    "            folder=self.config.test_data_path,\n",
    "            davis_root=self.config.davis_root,\n",
    "            n_frames=self.config.n_frames,\n",
    "            noise_sigma=self.config.noise_sigma,\n",
    "            patch_size=self.config.patch_size\n",
    "        )\n",
    "\n",
    "        # Windows-compatible DataLoader\n",
    "        num_workers = 0 if os.name == 'nt' else self.config.num_workers\n",
    "        pin_memory = False if os.name == 'nt' else True\n",
    "        \n",
    "        print(f\"üîß DataLoader settings: workers={num_workers}, pin_memory={pin_memory}\")\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory\n",
    "        )\n",
    "\n",
    "        print(f\"\\nüöÄ Testing TAP on {len(test_loader)} sequences...\")\n",
    "\n",
    "        psnr_results = {'noisy': [], 'denoised': []}\n",
    "        ssim_results = {'noisy': [], 'denoised': []}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(tqdm(test_loader, desc=\"üé¨ Testing TAP\")):\n",
    "                noisy_frames = batch['noisy_frames'].to(self.device)\n",
    "                center_clean = batch['center_clean'].to(self.device)\n",
    "                center_noisy = batch['center_noisy'].to(self.device)\n",
    "                video_names = batch['video_name']\n",
    "                frame_idxs = batch['frame_idx']\n",
    "\n",
    "                try:\n",
    "                    denoised = self.model(noisy_frames)\n",
    "\n",
    "                    for i in range(len(video_names)):\n",
    "                        video_name = video_names[i]\n",
    "                        frame_idx = frame_idxs[i].item()\n",
    "\n",
    "                        clean_np = np.clip(center_clean[i].permute(1, 2, 0).cpu().numpy(), 0, 1)\n",
    "                        noisy_np = np.clip(center_noisy[i].permute(1, 2, 0).cpu().numpy(), 0, 1)\n",
    "                        denoised_np = np.clip(denoised[i].permute(1, 2, 0).cpu().numpy(), 0, 1)\n",
    "\n",
    "                        psnr_noisy = compare_psnr(clean_np, noisy_np, data_range=1.0)\n",
    "                        psnr_denoised = compare_psnr(clean_np, denoised_np, data_range=1.0)\n",
    "                        ssim_noisy = compare_ssim(clean_np, noisy_np, channel_axis=2, data_range=1.0)\n",
    "                        ssim_denoised = compare_ssim(clean_np, denoised_np, channel_axis=2, data_range=1.0)\n",
    "\n",
    "                        psnr_results['noisy'].append(psnr_noisy)\n",
    "                        psnr_results['denoised'].append(psnr_denoised)\n",
    "                        ssim_results['noisy'].append(ssim_noisy)\n",
    "                        ssim_results['denoised'].append(ssim_denoised)\n",
    "\n",
    "                        if self.config.save_visualizations and batch_idx < 5:\n",
    "                            viz_path = os.path.join(self.viz_dir, f'{video_name}_frame{frame_idx}.png')\n",
    "                            visualize_results(\n",
    "                                center_noisy[i], denoised[i], center_clean[i],\n",
    "                                viz_path, video_name, frame_idx,\n",
    "                                display=False\n",
    "                            )\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error processing batch {batch_idx}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        # Print results\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üìä TRUE TAP TEST RESULTS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if psnr_results['noisy']:  # Check if we have results\n",
    "            print(f\"\\nüìà OVERALL PERFORMANCE:\")\n",
    "            print(f\"   Noisy PSNR:    {np.mean(psnr_results['noisy']):.2f} dB\")\n",
    "            print(f\"   Denoised PSNR: {np.mean(psnr_results['denoised']):.2f} dB\")\n",
    "            print(f\"   Improvement:   +{np.mean(psnr_results['denoised']) - np.mean(psnr_results['noisy']):.2f} dB\")\n",
    "            print(f\"   Noisy SSIM:    {np.mean(ssim_results['noisy']):.4f}\")\n",
    "            print(f\"   Denoised SSIM: {np.mean(ssim_results['denoised']):.4f}\")\n",
    "\n",
    "            # Save results\n",
    "            results_file = os.path.join(self.config.output_dir, 'test_results.txt')\n",
    "            with open(results_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(\"TRUE TAP TEST RESULTS\\n\")\n",
    "                f.write(\"=\"*50 + \"\\n\")\n",
    "                f.write(f\"Overall PSNR (Noisy): {np.mean(psnr_results['noisy']):.2f} dB\\n\")\n",
    "                f.write(f\"Overall PSNR (Denoised): {np.mean(psnr_results['denoised']):.2f} dB\\n\")\n",
    "                f.write(f\"Overall SSIM (Noisy): {np.mean(ssim_results['noisy']):.4f}\\n\")\n",
    "                f.write(f\"Overall SSIM (Denoised): {np.mean(ssim_results['denoised']):.4f}\\n\")\n",
    "\n",
    "            print(f\"\\nüìÑ Results saved to: {results_file}\")\n",
    "            print(f\"üñºÔ∏è  Visualizations saved in: {self.viz_dir}\")\n",
    "        else:\n",
    "            print(\"‚ùå No valid test results generated!\")\n",
    "\n",
    "# -------------------------\n",
    "# Main Function (UPDATED FOR WINDOWS)\n",
    "# -------------------------\n",
    "def main():\n",
    "    print(\"=\"*70)\n",
    "    print(\"üöÄ TRUE TAP MODEL TESTING - VS CODE\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nüéØ FLOW:\")\n",
    "    print(\"   Noisy -> Pre-trained Denoiser -> TAP Temporal Fusion -> Denoised\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    config = TestConfig()\n",
    "\n",
    "    # Try locating test data\n",
    "    possible_test_paths = [\n",
    "        './selected_videos/test',\n",
    "        './selected_videos/val', \n",
    "        './selected_videos/train',  # Fallback to train if needed\n",
    "        './davis_data/DAVIS/JPEGImages/480p'\n",
    "    ]\n",
    "\n",
    "    for test_path in possible_test_paths:\n",
    "        if os.path.exists(test_path):\n",
    "            config.test_data_path = test_path\n",
    "            print(f\"‚úÖ Using test data from: {test_path}\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No test data folder found. Will use synthetic data.\")\n",
    "\n",
    "    tester = TAPTester(config)\n",
    "    tester.test_model()\n",
    "\n",
    "    print(\"\\n‚úÖ Testing completed!\")\n",
    "    print(f\"üìÅ Results saved in: {config.output_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set seeds for reproducibility\n",
    "    random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 37547.883844,
   "end_time": "2025-11-03T01:32:53.086191",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-02T15:07:05.202347",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
